<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>6.3. Choice of K &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6.4. Hierarchical Clustering" href="w6_4_hcluster.html" />
    <link rel="prev" title="6.2. K-means and K-medoids" href="w6_2_kmeans.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../w1/w1_index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w4/w4_index.html">4. Regression Trees and Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w5/w5_index.html">5. Nonlinear Regression</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="w6_index.html">6. Clustering Analysis</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="w6_1_distance.html">6.1. Distance Measures</a></li>
<li class="toctree-l2"><a class="reference internal" href="w6_2_kmeans.html">6.2. K-means and K-medoids</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">6.3. Choice of K</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">6.3.1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gap-statistics">6.3.2. Gap Statistics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#generating-data-from-the-reference-distribution">Generating Data from the Reference Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="#determining-optimal-k-with-gap-statistic">Determining Optimal K with Gap Statistic</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#silhouette-statistics">6.3.3. Silhouette Statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="#prediction-strength">6.3.4. Prediction Strength</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#defining-prediction-strength">Defining Prediction Strength</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="w6_4_hcluster.html">6.4. Hierarchical Clustering</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w7/w7_index.html">7. Latent Structure Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w9/w9_index.html">9. Discriminant Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w11/w11_index.html">11. Support Vector Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w6_index.html"><span class="section-number">6. </span>Clustering Analysis</a></li>
      <li class="breadcrumb-item active"><span class="section-number">6.3. </span>Choice of K</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="choice-of-k">
<h1><span class="section-number">6.3. </span>Choice of K<a class="headerlink" href="#choice-of-k" title="Link to this heading"></a></h1>
<section id="introduction">
<h2><span class="section-number">6.3.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>In supervised learning, the goal is clear: make accurate predictions for the target variable, Y. But unsupervised learning, such as clustering, doesn’t have a Y variable, making it challenging to evaluate its accuracy or effectiveness. This lack of a clear target introduces complexities when determining the optimal number of clusters, K.</p>
<p>In supervised scenarios like regression, the go-to method for tuning parameters is cross-validation. However, applying cross-validation directly to clustering isn’t straightforward. Despite these challenges, several techniques aid in determining the optimal K. Key among them are gap statistics, silhouette statistics, and prediction strength.</p>
</section>
<section id="gap-statistics">
<h2><span class="section-number">6.3.2. </span>Gap Statistics<a class="headerlink" href="#gap-statistics" title="Link to this heading"></a></h2>
<p>When examining clustering effectiveness, many measures gauge the compactness or tightness of clusters. A common metric is the within cluster sum of squares, which, when based on the L2 distance, matches the objective function of K-means.</p>
<div class="math notranslate nohighlight">
\[SS(K) = \sum_{k=1}^K \sum_{z_i=k} \| x_i - m_k\|^2.\]</div>
<p>It’s natural to aim for a smaller SS, indicating tighter clusters. However, as the number of clusters (K) increases, the SS inherently decreases for the same dataset. Thus, relying solely on SS can be misleading when selecting the optimal K.</p>
<p>To determine the optimal K, researchers often use the “elbow method.” Here, the sum of squares is plotted against K. If a curve is observed with a distinct “elbow” point, that point often signifies the best K value. However, in real-world data, identifying the precise elbow can be challenging due to noise and complexity.</p>
<p>The <strong>gap statistic</strong> (Tibshirani, Walther and Hastie, 2001) compares the clustering of actual data against a random clustering from a reference distribution. It’s calculated by measuring the SS from the observed data against the expected log sum of squares from a reference set. This reference set is derived from a distribution that has no intrinsic clustering, meaning an ideal number of clusters would be one.</p>
<div class="math notranslate nohighlight">
\[\begin{split}G(K) &amp;= \textcolor{red}{\mathbb{E}_0} \Big [ \log SS^*(K) \Big ]- \log SS_{\text{obs}}(K) \\
        &amp; \approx  \frac{1}{B} \sum_{b=1}^B \log SS^*_b(K) - \log SS_{\text{obs}}(K)\end{split}\]</div>
<p>To estimate the gap statistic, multiple samples from the reference distribution are taken, and the average over these samples provides an expectation. As K grows, even though the sum of squares shrinks, the difference (or gap) may not always decrease. A high gap statistic suggests that the SS for the observed data at a particular K is notably smaller than its reference counterpart, indicating good clustering.</p>
<section id="generating-data-from-the-reference-distribution">
<h3>Generating Data from the Reference Distribution<a class="headerlink" href="#generating-data-from-the-reference-distribution" title="Link to this heading"></a></h3>
<p>There are two proposed methods:</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>Uniform Sampling:</dt><dd><p>Here, the reference data is uniformly sampled over the range of the observed data. This method may not be effective if the observed data has distinct shapes.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Principal Component Based Sampling:</dt><dd><p>This method samples over the range of the principal components of the observed data, ensuring better alignment with the data’s structure.</p>
</dd>
</dl>
</li>
</ol>
</section>
<section id="determining-optimal-k-with-gap-statistic">
<h3>Determining Optimal K with Gap Statistic<a class="headerlink" href="#determining-optimal-k-with-gap-statistic" title="Link to this heading"></a></h3>
<p>Plot the gap statistic values for different K.</p>
<p>The optimal K is determined either by identifying the highest gap statistic or, in a sequential approach, by selecting the first K where its gap statistic exceeds that of K+1.</p>
<p>Since the gap statistic is based on random sampling, there’s inherent variability. One-standard-error principle is used to account for this uncertainty. We compare the gap statistic at K to the lower bound of the gap statistic for K+1 (subtracting one standard error). If the former is greater, we consider that K as optimal.</p>
<div class="math notranslate nohighlight">
\[K_{\text{opt}} = \arg\min_K \{K : G(K) \ge G(K+1) - s_{K+1} \}\]</div>
<p>where <span class="math notranslate nohighlight">\(s_K = \text{sd}_0(\log SS(K)) \sqrt{1+1/B}\)</span>.</p>
</section>
</section>
<section id="silhouette-statistics">
<h2><span class="section-number">6.3.3. </span>Silhouette Statistics<a class="headerlink" href="#silhouette-statistics" title="Link to this heading"></a></h2>
<p>The Silhouette statistic (Rousseeuw, 1987) of the i-th obs measures how well it fits in its own cluster versus how well it fits in its next closest cluster.</p>
<a class="reference internal image-reference" href="../_images/w6_sil_def.png"><img alt="../_images/w6_sil_def.png" src="../_images/w6_sil_def.png" style="width: 562.5px; height: 186.0px;" /></a>
<p>For well-clustered observations where <span class="math notranslate nohighlight">\(a_i\)</span> is significantly smaller than <span class="math notranslate nohighlight">\(b_i\)</span>, <span class="math notranslate nohighlight">\(s_i\)</span> approaches 1. However, if the i-th observation lies on the borderline of two clusters, both <span class="math notranslate nohighlight">\(a_i\)</span> and <span class="math notranslate nohighlight">\(b_i\)</span>, will be similar, rendering <span class="math notranslate nohighlight">\(s_i\)</span> close to 0.
In the K-means scenario, <span class="math notranslate nohighlight">\(b_i\)</span> should be greater than <span class="math notranslate nohighlight">\(a_i\)</span>; otherwise, the observation would belong to a different cluster. However, using other clustering algorithms, <span class="math notranslate nohighlight">\(b_i\)</span> could be smaller than <span class="math notranslate nohighlight">\(a_i\)</span>, potentially resulting in a negative silhouette statistic. Even then, the value of <span class="math notranslate nohighlight">\(s_i\)</span> would remain above -1. Extremely negative silhouette statistics indicate poor clustering for the i-th observation.</p>
<p>To evaluate the overall quality of the clustering, we seek a large silhouette statistic across all observations. The Silhouette Coefficient (SC), sometimes referred to as the silhouette width, is defined as the average silhouette statistic over all samples.</p>
<p>There are established benchmarks for the silhouette coefficient to gauge the quality of the identified clustering. For example, a rule of thumb from Anja et al. “Clustering in an Object-Oriented Environment ”:</p>
<ul class="simple">
<li><p>SC &gt; 70%:  A strong structure has been found.</p></li>
<li><p>SC &gt; 50%: A reasonable structure has been found.</p></li>
<li><p>SC &gt; 26% : The structure is weak and could be artificial, try additional methods.</p></li>
<li><p>SC &lt; 26%: No substantial structure has been found.</p></li>
</ul>
<p>We can also plot the silhouette coefficient against varying K values. The optimal K can either be the one yielding the largest silhouette coefficient or a K surpassing a specific threshold.</p>
<p>For a comprehensive look into our methodology, including the specific R commands and outputs, please refer to our code page.</p>
</section>
<section id="prediction-strength">
<h2><span class="section-number">6.3.4. </span>Prediction Strength<a class="headerlink" href="#prediction-strength" title="Link to this heading"></a></h2>
<p>The Prediction Strength method is another strategy for choosing the optimal number of clusters K. Here’s how it works:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Step 0:</dt><dd><p>Split the dataset into two subsets: A (the training set) and B (the test set).</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Step 1:</dt><dd><p>Assume there are m observations in test set B. Cluster these m samples into K clusters using your chosen clustering algorithm. Label these clusters as <span class="math notranslate nohighlight">\(C_1, \dots, C_K\)</span>. Each cluster <span class="math notranslate nohighlight">\(C_j\)</span> contains <span class="math notranslate nohighlight">\(m_j\)</span> observations. The sum of the sizes of these clusters <span class="math notranslate nohighlight">\(m_1, \dots, m_K\)</span> is equal to m. This step determines the <strong>true</strong> clustering structure for the test data.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Step 2:</dt><dd><p>Cluster the training data A into K clusters. Use the resultant clustering rule to allocate the m observations from test set B into these K clusters. This essentially means <strong>predicting</strong> the cluster membership for the test set based on the cluster centers determined from the training data. Denote these predicted clusters as <span class="math notranslate nohighlight">\(C'_1, \dots, C'_K\)</span>.</p>
</dd>
</dl>
</li>
</ul>
<p>A direct comparison between the true clusters (<span class="math notranslate nohighlight">\(C_j\)</span>) and predicted clusters (<span class="math notranslate nohighlight">\(C'_j\)</span>) is challenging due to the relabeling problem. The identity of clusters may change between datasets, so the clusters from Step 1 and Step 2 might not correspond directly.</p>
<p>A technique to address this is to assess prediction error using the association or co-membership matrix. This matrix for m observations is m-by-m. An entry in this matrix is “1” if observations ‘i’ and ‘j’ belong to the same cluster, otherwise “0”. For both steps, an association matrix can be generated.</p>
<section id="defining-prediction-strength">
<h3>Defining Prediction Strength<a class="headerlink" href="#defining-prediction-strength" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>For each cluster <span class="math notranslate nohighlight">\(C_j\)</span> obtained in Step 1, examine every pair of observations within that cluster. Check if those pairs remain in the same cluster in Step 2.</p></li>
<li><p>Calculate the average accuracy for each cluster, indicating how often pairs of observations remain together.</p></li>
<li><p>The “prediction strength” (Tibshirani and Walther 2005) for K clusters is the worst (smallest) accuracy among these clusters.</p></li>
</ul>
<p>Choosing the worst accuracy avoids potential biases. If one were to take the average accuracy, the method might be skewed by predominantly classifying many observations into one cluster, artificially inflating the average.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w6_2_kmeans.html" class="btn btn-neutral float-left" title="6.2. K-means and K-medoids" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w6_4_hcluster.html" class="btn btn-neutral float-right" title="6.4. Hierarchical Clustering" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>