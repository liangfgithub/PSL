<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>11.2. The Separable case &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11.3. The Non-separable case" href="w11_2_non_separable_case.html" />
    <link rel="prev" title="11.1. Introduction" href="w11_0_intro.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../w1/w1_index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w4/w4_index.html">4. Regression Trees and Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w5/w5_index.html">5. Nonlinear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w6/w6_index.html">6. Clustering Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w7/w7_index.html">7. Latent Structure Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w9/w9_index.html">9. Discriminant Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="w11_index.html">11. Support Vector Machine</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="w11_0_intro.html">11.1. Introduction</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">11.2. The Separable case</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#the-max-margin-problem">11.2.1. The Max-Margin Problem</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-kkt-conditions">11.2.2. The KKT Conditions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#optimization-without-constraints">Optimization Without Constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="#optimization-with-equality-constraints">Optimization With Equality Constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="#optimization-with-inequality-constraints">Optimization With Inequality Constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="#kkt-conditions-for-svm">KKT Conditions for SVM</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#the-duality">11.2.3. The Duality</a></li>
<li class="toctree-l3"><a class="reference internal" href="#prediction">11.2.4. Prediction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#computation-cost">Computation Cost</a></li>
<li class="toctree-l4"><a class="reference internal" href="#probabilistic-outputs">Probabilistic Outputs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#summary">11.2.5. Summary</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="w11_2_non_separable_case.html">11.3. The Non-separable case</a></li>
<li class="toctree-l2"><a class="reference internal" href="w11_3_practical_issues.html">11.4. Practical Issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="w11_4_nonlinear.html">11.5. Nonlinear SVMs</a></li>
<li class="toctree-l2"><a class="reference internal" href="w11_5_summ.html">11.6. Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="w11_6_appendix.html">11.7. Appendix</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w11_index.html"><span class="section-number">11. </span>Support Vector Machine</a></li>
      <li class="breadcrumb-item active"><span class="section-number">11.2. </span>The Separable case</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-separable-case">
<h1><span class="section-number">11.2. </span>The Separable case<a class="headerlink" href="#the-separable-case" title="Link to this heading"></a></h1>
<p>In Support Vector Machine (SVM), we aim to find a linear decision boundary, but unlike Linear Discriminant Analysis (LDA) and logistic regression, our focus isn’t on modeling conditional or joint distributions. Instead, we are directly modeling the decision boundary.</p>
<section id="the-max-margin-problem">
<h2><span class="section-number">11.2.1. </span>The Max-Margin Problem<a class="headerlink" href="#the-max-margin-problem" title="Link to this heading"></a></h2>
<p>To illustrate this, let’s consider a scenario where we have two groups of points, and we want to create a linear decision boundary to separate them. Our goal is to maximize the separation, making the margin between the two groups as wide as possible.</p>
<a class="reference internal image-reference" href="../_images/w11_linear_sep_1.png"><img alt="../_images/w11_linear_sep_1.png" src="../_images/w11_linear_sep_1.png" style="width: 261.3px; height: 176.15px;" /></a>
<p>To achieve this, we introduce a solid blue line to separate the two groups of points, and we imagine creating parallel dashed lines on either side of it. We extend these dashed lines until they touch the green/red points on either side, creating a “margin” between them. This margin is essentially an “avenue” that separates the two groups, and we aim to maximize its width.</p>
<p>To formulate this problem mathematically, we start by representing the linear decision boundary (represented by the solid blue line) using coefficients, such as the slope <span class="math notranslate nohighlight">\(\beta\)</span> and the intercept <span class="math notranslate nohighlight">\(\beta_0.\)</span> However, <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\beta_0\)</span> are not uniquely determined, as we can scale them or flip their signs and still have the same line. To address this, we take the following steps to fix these parameters.</p>
<ul class="simple">
<li><p>We set the output labels <span class="math notranslate nohighlight">\(y\)</span> to be either +1 or -1. Then require <span class="math notranslate nohighlight">\(y_i (\beta \cdot x_i + \beta_0)\)</span> should always be positive. Here  <span class="math notranslate nohighlight">\(\beta \cdot x_i = \beta^t x_i\)</span> represents the Euclidean inner product between two vectors.</p></li>
<li><p>We also need to fix the scale of <span class="math notranslate nohighlight">\(\beta\)</span> and <span class="math notranslate nohighlight">\(\beta_0.\)</span> To do this, we parameterize the two dashed lines on either side of the solid blue line as <span class="math notranslate nohighlight">\(\beta \cdot x + \beta_0 = 1\)</span> and <span class="math notranslate nohighlight">\(\beta \cdot x + \beta_0 = -1\)</span>. This scaling fixes the magnitude of <span class="math notranslate nohighlight">\(\beta.\)</span></p></li>
</ul>
<a class="reference internal image-reference" href="../_images/w11_linear_sep_2.png"><img alt="../_images/w11_linear_sep_2.png" src="../_images/w11_linear_sep_2.png" style="width: 245.70000000000002px; height: 155.35px;" /></a>
<p>Next, our objective is to measure the margin or the width of the Avenue between the two groups of points. Note that the distance between the solid and dashed lines is half the width. Recognize that the slope parameter of the line <span class="math notranslate nohighlight">\(\beta\)</span> is orthogonal to both the solid and dashed lines.</p>
<a class="reference internal image-reference" href="../_images/w11_linear_margin.png"><img alt="../_images/w11_linear_margin.png" src="../_images/w11_linear_margin.png" style="width: 300.3px; height: 176.8px;" /></a>
<p>To calculate the width of the Avenue, we can pick a point on a dashed line, denoted as <span class="math notranslate nohighlight">\(z\)</span>, and another point on the solid line, denoted as <span class="math notranslate nohighlight">\(x\)</span>. We then compute the projection of the vector from <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(z\)</span> onto the direction of <span class="math notranslate nohighlight">\(\beta\)</span>  to find the magnitude of the red bar, representing half of the Avenue’s width.</p>
<div class="math notranslate nohighlight">
\[(x - z)^t \frac{\beta}{\| \beta\|} = \frac{x^t \beta - z^t \beta}{\| \beta\|} = \frac{1}{\| \beta\|}.\]</div>
<p>This leads us to the conclusion that half of the Avenue’s width is equal to <span class="math notranslate nohighlight">\(1/\|\beta\|.\)</span> Therefore, to maximize the margin, we can equivalently minimize <span class="math notranslate nohighlight">\(\|\beta\|^2/2.\)</span></p>
<p>So, our goal becomes an optimization problem (know as the <strong>max-margin</strong> problem) subject to certain constraints. These constraints ensure that the data points fall on the correct side of the dashed lines.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\min_{\beta, \beta_0 } &amp; \frac{1}{2} \|\beta\|^2  \\
\text{subject to } &amp; y_i ( \beta \cdot x_i + \beta_0) - 1 \ge 0,\end{split}\]</div>
<p>The max-margin problem is a convex optimization problem with a quadratic objective function and linear or affine constraints, making it free from issues related to local optima.</p>
<p>Additionally, the equation above represents the primal problem in SVM. Instead of directly solving the primal problem, we often solve its dual problem. The dual problem yields a set of <span class="math notranslate nohighlight">\(\lambda\)</span> values, which can be linked to the solutions of the primal problem using the Karush-Kuhn-Tucker (KKT) conditions.</p>
</section>
<section id="the-kkt-conditions">
<h2><span class="section-number">11.2.2. </span>The KKT Conditions<a class="headerlink" href="#the-kkt-conditions" title="Link to this heading"></a></h2>
<p>The Karush-Kuhn-Tucker (KKT) conditions are a set of necessary conditions that characterize the solutions to constrained optimization problems.</p>
<p>When we want to minimize a function <span class="math notranslate nohighlight">\(f(x),\)</span> we focus on its derivative. Specifically, we look at the negative derivative at a particular point, <span class="math notranslate nohighlight">\(- \frac{\partial f(x)}{\partial x}\)</span>, which indicates a direction along which we can make further reductions in the value of <span class="math notranslate nohighlight">\(f\)</span>.</p>
<section id="optimization-without-constraints">
<h3>Optimization Without Constraints<a class="headerlink" href="#optimization-without-constraints" title="Link to this heading"></a></h3>
<p>First, let’s consider a scenario without constraints. We know that at the minimizer, the derivative of
<span class="math notranslate nohighlight">\(f\)</span> evaluated at that point should be zero. In mathematical terms, this is expressed as <span class="math notranslate nohighlight">\(\frac{\partial f(x)}{\partial x} = 0\)</span>. This condition signifies that there is no direction left to further reduce the value of <span class="math notranslate nohighlight">\(f\)</span>, making it a necessary condition for optimality.</p>
</section>
<section id="optimization-with-equality-constraints">
<h3>Optimization With Equality Constraints<a class="headerlink" href="#optimization-with-equality-constraints" title="Link to this heading"></a></h3>
<p>Next, let’s extend this to situations where we have an equality constraint:</p>
<div class="math notranslate nohighlight">
\[\min_x  f(x), \quad \text{subject to }  g(x) =  b.\]</div>
<p>Here, our <strong>feasible region</strong> is defined by the set of x values that satisfy <span class="math notranslate nohighlight">\(g(x) =  b.\)</span></p>
<p>Suppose x is a solution to this problem. At such a point, the negative derivative of <span class="math notranslate nohighlight">\(f\)</span>, which indicates a direction along which we can further reduce the value of <span class="math notranslate nohighlight">\(f\)</span>, may not necessarily be zero. However, this non-zero direction must be a “forbidden” one in the sense that moving along it would lead us outside the feasible region, violating the equality constraint. Mathematically, we can express this situation as:
<span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x} + \lambda \frac{\partial g}{\partial x} = 0,\)</span> where <span class="math notranslate nohighlight">\(\lambda\)</span> can take any real number value.</p>
<a class="reference internal image-reference" href="../_images/w11_optimize.png"><img alt="../_images/w11_optimize.png" src="../_images/w11_optimize.png" style="width: 383.59999999999997px; height: 198.1px;" /></a>
</section>
<section id="optimization-with-inequality-constraints">
<h3>Optimization With Inequality Constraints<a class="headerlink" href="#optimization-with-inequality-constraints" title="Link to this heading"></a></h3>
<p>Now, consider scenarios with an inequality constraint:</p>
<div class="math notranslate nohighlight">
\[\min_x  f(x), \quad \text{subject to }  g(x) =  b.\]</div>
<p>Assuming <span class="math notranslate nohighlight">\(g(x)\)</span> is a concave function and <span class="math notranslate nohighlight">\(f(x)\)</span> is smooth, there are two cases to explore. Suppose x is a solution to this problem.</p>
<ul>
<li><dl class="simple">
<dt>Case 1:</dt><dd><p>x is inside the convex feasible region  (interior point): In this case, we can place a small neighborhood around x where the entire neighborhood remains within the feasible region. Then, the derivative <span class="math notranslate nohighlight">\(\frac{\partial f(x)}{\partial x}\)</span> must be zero at this point. This aligns with the idea that there are no directions within the feasible region where <span class="math notranslate nohighlight">\(f\)</span> can be further reduced.</p>
</dd>
</dl>
</li>
<li><dl>
<dt>Case 2:</dt><dd><p>x lies on the boundary of the convex region (boundary point): Here, the negative derivative <span class="math notranslate nohighlight">\(-\frac{\partial f(x)}{\partial x}\)</span> may not necessarily be zero. However, this direction is a “forbidden” one in the sense that moving along it would decrease the value of <span class="math notranslate nohighlight">\(g\)</span>, leading to <span class="math notranslate nohighlight">\(g(x) &lt; b\)</span> and therefore violating the inequality constraint. Mathematically, we can describe this direction as</p>
<div class="math notranslate nohighlight">
\[- \frac{\partial f(x)}{\partial x} = - \lambda \frac{\partial g}{\partial x}, \quad \lambda \ge 0,\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> must always be non-negative due to the inequality constraint.</p>
</dd>
</dl>
</li>
</ul>
<p>The conditions outlined above can be encapsulated in the KKT conditions for optimization with inequality constraints.</p>
<a class="reference internal image-reference" href="../_images/w11_KKT_general.png"><img alt="../_images/w11_KKT_general.png" src="../_images/w11_KKT_general.png" style="width: 183.39999999999998px; height: 219.79999999999998px;" /></a>
<p>Specifically, we have four conditions within the red box:</p>
<ol class="arabic simple">
<li><p>First Condition: The derivative of the Lagrangian function should be equal to zero. Here the Lagrangian function is defined as</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}L(x, \lambda) &amp; = f(x) - \lambda (g(x) - b) \\
\frac{\partial}{\partial x} L &amp; = 0 \quad \Longrightarrow \quad \frac{\partial f(x)}{\partial x} =  \lambda \frac{\partial g(x)}{\partial x}.\end{split}\]</div>
<ol class="arabic simple" start="2">
<li><p>Second Condition: <span class="math notranslate nohighlight">\(\lambda \ge 0.\)</span></p></li>
<li><p>Third Condition: The inequality constraints <span class="math notranslate nohighlight">\(g(x) \ge b\)</span> should be satisfied.</p></li>
<li><p>Fourth Condition (Complementary Slackness): This condition ensures that when the constraint is not active, i.e., <span class="math notranslate nohighlight">\(g(x) - b \ne 0,\)</span> then the Lagrange multiplier <span class="math notranslate nohighlight">\(\lambda\)</span> should be zero (Case 1).</p></li>
</ol>
</section>
<section id="kkt-conditions-for-svm">
<h3>KKT Conditions for SVM<a class="headerlink" href="#kkt-conditions-for-svm" title="Link to this heading"></a></h3>
<p>The Lagrangian function and the four KKT conditions for the SVM problem are summarized below. The Lagrangian function involves two sets of arguments: the first set is for the primal problem, which includes the slope <span class="math notranslate nohighlight">\(\beta\)</span> and intercept <span class="math notranslate nohighlight">\(\beta_0\)</span>, and the second set is for the Lagrange multipliers <span class="math notranslate nohighlight">\(\lambda_1\)</span> to <span class="math notranslate nohighlight">\(\lambda_n\)</span>, as we have <span class="math notranslate nohighlight">\(n\)</span> inequality constraints.</p>
<a class="reference internal image-reference" href="../_images/w11_KKT_SVM.png"><img alt="../_images/w11_KKT_SVM.png" src="../_images/w11_KKT_SVM.png" style="width: 509.59999999999997px; height: 213.5px;" /></a>
</section>
</section>
<section id="the-duality">
<h2><span class="section-number">11.2.3. </span>The Duality<a class="headerlink" href="#the-duality" title="Link to this heading"></a></h2>
<p>Next, let’s discuss the duality between the primal and dual problems in optimization.</p>
<p>We start with the general <strong>primal problem</strong>, where our goal is to minimize a function <span class="math notranslate nohighlight">\(f(x)\)</span> subject to an inequality constraint of the form <span class="math notranslate nohighlight">\(g(x) \geq b.\)</span> To establish the duality, we introduce the Lagrangian of this problem: <span class="math notranslate nohighlight">\(L(x, \lambda) = f(x) - \lambda (g(x) - b)\)</span>. (There is a typo in the image below: the Lagrangian multiplier <span class="math notranslate nohighlight">\(\lambda \ge 0\)</span>.)</p>
<a class="reference internal image-reference" href="../_images/w11_primal_minimax.png"><img alt="../_images/w11_primal_minimax.png" src="../_images/w11_primal_minimax.png" style="width: 370.29999999999995px; height: 283.5px;" /></a>
<p>The primal problem can be transformed into a minimax problem, which means minimizing with respect to variable <span class="math notranslate nohighlight">\(x\)</span> and maximizing with respect to the Lagrange multiplier <span class="math notranslate nohighlight">\(\lambda\)</span>. To understand their equivalence, we analyze what’s inside the max operation with respect to <span class="math notranslate nohighlight">\(\lambda.\)</span> Two cases emerge:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(g(x) \geq b,\)</span> the expression inside the parentheses is positive. Therefore, we set <span class="math notranslate nohighlight">\(\lambda = 0,\)</span> which leads to the maximized value being equivalent to <span class="math notranslate nohighlight">\(f(x).\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(g(x) &lt; b,\)</span> the expression inside the parentheses is negative, and the product of a negative value and <span class="math notranslate nohighlight">\((-\lambda)\)</span> becomes positive. In this scenario, we aim to maximize <span class="math notranslate nohighlight">\(\lambda\)</span> as much as possible, driving the value inside the parentheses to infinity. This situation results in the overall maximization value being infinite.</p></li>
</ul>
<p>By analyzing these two cases, we conclude that the expression inside the max operation simplifies to <span class="math notranslate nohighlight">\(f(x).\)</span> Therefore, the primal problem can be reformulated as a minimax problem.</p>
<p>Next, we switch the order of the minimization and maximization, the resulting optimization problem is known as  the <strong>dual problem</strong>:</p>
<a class="reference internal image-reference" href="../_images/w11_dual.png"><img alt="../_images/w11_dual.png" src="../_images/w11_dual.png" style="width: 259.0px; height: 108.5px;" /></a>
<p>Usually, the dual problem provides a lower bound on the primal problem (known as weak duality):</p>
<div class="math notranslate nohighlight">
\[\max_\lambda \min_x L(x, \lambda) \le \min_x \max_\lambda L(x, \lambda).\]</div>
<p>But for SVM-type convex programming problems, strong duality holds, which means the optimal value of the primal roblem is equal to the optimal value of the dual:</p>
<div class="math notranslate nohighlight">
\[\max_\lambda \min_x L(x, \lambda) = \min_x \max_\lambda L(x, \lambda).\]</div>
<p>The primal and dual problems for SVM are summarized below:</p>
<a class="reference internal image-reference" href="../_images/w11_linear_prime_dual.png"><img alt="../_images/w11_linear_prime_dual.png" src="../_images/w11_linear_prime_dual.png" style="width: 673.4px; height: 334.59999999999997px;" /></a>
<p>For SVM, we can choose to solve either the primal or dual roblem to obtain the same optimal solution. In practice, solving the dual problem is often preferred for various reasons.</p>
<ul class="simple">
<li><p>The dual  is easier to optimize than the primal, especially when it comes to handling constraints.</p></li>
<li><p>The dual solution involves Lagrange multipliers <span class="math notranslate nohighlight">\(\lambda_i\)</span>’s.  Many of these <span class="math notranslate nohighlight">\(\lambda_i\)</span> values will be zero for data points that are not support vectors (data points on the dashed lines). Complementary Slackness in the KKT conditions  ensures that only support vectors have non-zero <span class="math notranslate nohighlight">\(\lambda\)</span> values. This leads to a sparse solution.</p></li>
<li><p>The dual formulation of the SVM is advantageous when using kernel functions. The kernel trick allows SVMs to implicitly map data into higher-dimensional feature spaces without explicitly computing the mapping.</p></li>
</ul>
</section>
<section id="prediction">
<h2><span class="section-number">11.2.4. </span>Prediction<a class="headerlink" href="#prediction" title="Link to this heading"></a></h2>
<p>Solving for the <span class="math notranslate nohighlight">\(\lambda_i\)</span> values in the dual problem allows us to retrieve the parameters of the primal problem, specifically the slope <span class="math notranslate nohighlight">\(\beta\)</span> and the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span> of the optimal linear decision boundary.</p>
<p>We will use the two equations colored in pink in the KKT conditions. The first equality can help us to solve for <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<div class="math notranslate nohighlight">
\[\beta = \sum \lambda_i y_i x_i = \sum_{i \in \textcolor{red}{N_s}} \lambda_i y_i x_i,\]</div>
<p>where <span class="math notranslate nohighlight">\(N_s\)</span> represents the set of support vectors. Pick any support vector; since it’s on the dashed line, it must satisfy the equality <span class="math notranslate nohighlight">\(y_i ( \beta \cdot x_i + \beta_0) - 1 =0.\)</span> Thus we can solve for the intercept <span class="math notranslate nohighlight">\(\beta_0\)</span>. For computational stability, it is common to compute multiple <span class="math notranslate nohighlight">\(\beta_0\)</span> values based on different support vectors and then average them. This helps mitigate slight variations that may occur in practice.</p>
<p>When it comes to making predictions for new data points, we simply evaluate the linear equation, involving <span class="math notranslate nohighlight">\((\beta, \beta_0)\)</span>, and the new feature vector <span class="math notranslate nohighlight">\(x^*\)</span>, and examine the sign of the result. The sign determines whether we predict the new feature to belong to one class (+1) or the other class (-1).</p>
<section id="computation-cost">
<h3>Computation Cost<a class="headerlink" href="#computation-cost" title="Link to this heading"></a></h3>
<p>SVM computation revolves around the dual problem, focusing on solving for <span class="math notranslate nohighlight">\(\lambda_1\)</span> to <span class="math notranslate nohighlight">\(\lambda_n.\)</span> Consequently, the computational cost of SVM primarily depends on the sample size (n), not the original feature dimension (p). This impiles that even a straightforward SVM model with a relatively low dimensional feature space can demand substantial computational resources when working with a large dataset.</p>
</section>
<section id="probabilistic-outputs">
<h3>Probabilistic Outputs<a class="headerlink" href="#probabilistic-outputs" title="Link to this heading"></a></h3>
<p>Unlike logistic regression or discriminant analysis, SVM directly determines the decision boundary without providing probabilities. However, there is a way to obtain probability outcomes from the distance of a point to the binary decision boundary based on Platt Scaling</p>
<p><a class="reference external" href="https://liangfgithub.github.io/ref/SVM_Platt.pdf">A Note on Platt’s Probabilistic Outputs for Support Vector Machines</a> (Hsuan-Tien Lin et al., 2007)</p>
</section>
</section>
<section id="summary">
<h2><span class="section-number">11.2.5. </span>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<p>Let me provide a refined summary of our discussion regarding <strong>linear SVMs for separable cases</strong>.</p>
<p>To recap, we’ve been exploring linear SVMs in scenarios where data points can be cleanly separated by a linear function within the feature space X. This separability implies that multiple suitable linear functions could exist. Our goal has been to find the optimal linear function, or decision boundary, that <strong>maximizes the margin</strong> between two groups of data points.</p>
<p>Formally, this problem can be framed as a constrained optimization task. The constraints ensure that data points from both groups are correctly positioned on the respective sides of the decision boundary. However, instead of directly solving this constrained optimization problem, we’ve been addressing the <strong>dual problem</strong>, a pivotal concept in SVMs.</p>
<p>In the dual problem, the objective is to find the Lagrange multipliers (lambda values) associated with the original constraints. Importantly, due to a fundamental property called the <strong>Complementary Slackness</strong>, many of these lambda values will be zero. This condition, which is part of the Karush-Kuhn-Tucker (KKT) conditions, dictates that <span class="math notranslate nohighlight">\(\lambda_i\)</span> and the constraint cannot both be nonzero simultaneously. Therefore, only data points located on the dashed lines exhibit nonzero <span class="math notranslate nohighlight">\(\lambda_i\)</span> values. These special data points are referred to as <strong>support vectors</strong>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w11_0_intro.html" class="btn btn-neutral float-left" title="11.1. Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w11_2_non_separable_case.html" class="btn btn-neutral float-right" title="11.3. The Non-separable case" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>