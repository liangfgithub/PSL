<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>1.1.5. Bias and variance tradeoff &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.2. Least squares vs. nearest neighbors" href="w1_2_index.html" />
    <link rel="prev" title="1.1.4. A Glimpse of Learning Theory" href="w1_1_learning_theory.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="w1_index.html">1. Introduction</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="w1_1_index.html">1.1. Introduction to statistical learning</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="w1_1_type_of_learning.html">1.1.1. Types of statistical learning problems</a></li>
<li class="toctree-l3"><a class="reference internal" href="w1_1_challenge.html">1.1.2. Challenge of supervised learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="w1_1_curse.html">1.1.3. Curse of dimensionality</a></li>
<li class="toctree-l3"><a class="reference internal" href="w1_1_learning_theory.html">1.1.4. A Glimpse of Learning Theory</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">1.1.5. Bias and variance tradeoff</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-darts-game">The Darts Game</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bias-and-variance-tradeoff-in-statistical-learning">Bias and Variance Tradeoff in Statistical Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-complexity-and-flexibility">Model Complexity and Flexibility</a></li>
<li class="toctree-l4"><a class="reference internal" href="#strategies-to-address-bias-and-variance">Strategies to Address Bias and Variance</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="w1_2_index.html">1.2. Least squares vs. nearest neighbors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w4/w4_index.html">4. Regression Trees and Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w5/w5_index.html">5. Nonlinear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w6/w6_index.html">6. Clustering Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w7/w7_index.html">7. Latent Structure Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w9/w9_index.html">9. Discriminant Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w11/w11_index.html">11. Support Vector Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w1_index.html"><span class="section-number">1. </span>Introduction</a></li>
          <li class="breadcrumb-item"><a href="w1_1_index.html"><span class="section-number">1.1. </span>Introduction to statistical learning</a></li>
      <li class="breadcrumb-item active"><span class="section-number">1.1.5. </span>Bias and variance tradeoff</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/w1/w1_1_bias_variance_tradeoff.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="bias-and-variance-tradeoff">
<h1><span class="section-number">1.1.5. </span>Bias and variance tradeoff<a class="headerlink" href="#bias-and-variance-tradeoff" title="Link to this heading"></a></h1>
<section id="the-darts-game">
<h2>The Darts Game<a class="headerlink" href="#the-darts-game" title="Link to this heading"></a></h2>
<p>To understand the bias and variance tradeoff, let’s examine the performance of two players in a darts game.</p>
<a class="reference internal image-reference" href="../_images/w1_darts.png"><img alt="../_images/w1_darts.png" src="../_images/w1_darts.png" style="width: 286.5px; height: 162.0px;" /></a>
<ul class="simple">
<li><p>Player 1 consistently throws darts close together, but they consistently miss the target by aiming at the wrong point. This player exhibits low variance but high bias.</p></li>
<li><p>Player 2’s throws show a lot of variability, with darts landing both near and far from the target. However, these attempts are distributed around the correct target area, resulting in low bias but high variance.</p></li>
</ul>
<p>If we evaluate their performance by calculating the expected or average distance from the true center, both players achieve a similar overall performance, despite the differing distribution of errors between bias and variance.</p>
</section>
<section id="bias-and-variance-tradeoff-in-statistical-learning">
<h2>Bias and Variance Tradeoff in Statistical Learning<a class="headerlink" href="#bias-and-variance-tradeoff-in-statistical-learning" title="Link to this heading"></a></h2>
<p>In statistical learning, the total error in predictions can be attributed to two main sources: bias and variance.</p>
<a class="reference internal image-reference" href="../_images/w1_func_space.png"><img alt="../_images/w1_func_space.png" class="align-left" src="../_images/w1_func_space.png" style="width: 221.5px; height: 120.5px;" /></a>
<p>When learning a regression function or a classification function, we must work within a predefined function space, the blue circle. For example, this space may consist of linear functions, quadratic functions, or other predetermined structures. However, the “truth” may lie outside the blue circle, implying that even with infinite data, we cannot perfectly capture it.</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Bias</strong> is the gap between the truth and the best approximation achievable within the function space.</p></li>
<li><p><strong>Variance</strong> reflects the fluctuations of our learned function within the function space. We often denote the learned function as f_n, where the subscript ‘n’ indicating its randomness, which arises because the learned function can vary based on the sample of data it’s trained on.</p></li>
</ul>
</div></blockquote>
<p>The relationship is clear: if the function space is limited, bias tends to be high while variance is low. Conversely, selecting a larger function space reduces bias but increases variance because f_n can vary with a much larger range of possibilities. Striking the right balance between bias and variance is a crucial aspect of statistical learning.</p>
</section>
<section id="model-complexity-and-flexibility">
<h2>Model Complexity and Flexibility<a class="headerlink" href="#model-complexity-and-flexibility" title="Link to this heading"></a></h2>
<p>Model complexity is often measured by the flexibility of the model. For instance:</p>
<ul class="simple">
<li><p>Linear model with two predictors (low complexity)</p></li>
<li><p>Linear model with ten predictors (medium complexity)</p></li>
<li><p>Model with quadratic terms (high complexity)</p></li>
</ul>
<p>As model complexity increases (i.e., the function space expands), there’s a trade-off: bias decreases while variance increases. Test error, which is a combination of both bias and variance, typically follows a U-shaped curve, with the optimal point lying somewhere in the middle. At one extreme, we find models of low complexity, leading to high bias but low variance. At the other end, high-complexity models result in low bias but high variance.</p>
<a class="reference internal image-reference" href="../_images/w1_U_shape_book.png"><img alt="../_images/w1_U_shape_book.png" src="../_images/w1_U_shape_book.png" style="width: 307.5px; height: 218.0px;" /></a>
<p>Interestingly, in the era of deep learning, researchers have observed that the performance curve may exhibit a ‘double descent’ shape, showing another minimum in error as complexity increases beyond the traditional optimum. This intriguing behavior challenges traditional understanding. We will explore this double descent behavior in an upcoming coding assignment.</p>
</section>
<section id="strategies-to-address-bias-and-variance">
<h2>Strategies to Address Bias and Variance<a class="headerlink" href="#strategies-to-address-bias-and-variance" title="Link to this heading"></a></h2>
<p>We will learn various flexible modeling techniques to reduce bias. Reducing bias is crucial because it enables us to approach the true function when provided with abundant data. However, this reduction in bias comes at the cost of increased variance. Therefore, we need to adopt strategies that strike the right balance between bias and variance:</p>
<ol class="arabic simple">
<li><p><strong>Regularization</strong>: This approach starts with a complex model and progressively reduces its parameters, corresponding to nested function spaces. Different values of lambda, a tuning parameter, control the level of regularization. An appropriate choice of lambda can be determined adaptively based on the data.</p></li>
<li><p><strong>Ensemble Methods</strong>: Ensemble methods combine multiple models from the low bias, high variance side to reduce variance. Averaging is a common technique for reducing variance.</p></li>
</ol>
<section id="final-note">
<h3>Final Note<a class="headerlink" href="#final-note" title="Link to this heading"></a></h3>
<p>While this course primarily focuses on prediction and statistical learning, data scientists often use predictive models for diverse purposes, including exploration, decision-making, and generating actionable insights. In certain scenarios, simpler and more interpretable models are preferred over complex black-box models.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w1_1_learning_theory.html" class="btn btn-neutral float-left" title="1.1.4. A Glimpse of Learning Theory" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w1_2_index.html" class="btn btn-neutral float-right" title="1.2. Least squares vs. nearest neighbors" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>