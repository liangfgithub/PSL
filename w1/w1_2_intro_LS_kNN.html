<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>1.2.1. Introduction to LS and kNN &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="1.2.2. Simulation Study" href="w1_2_simulation_study.html" />
    <link rel="prev" title="1.2. Least squares vs. nearest neighbors" href="w1_2_index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="w1_index.html">1. Introduction</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="w1_1_index.html">1.1. Introduction to statistical learning</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="w1_2_index.html">1.2. Least squares vs. nearest neighbors</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">1.2.1. Introduction to LS and kNN</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#k-nearest-neighbor-knn">k-Nearest Neighbor (kNN)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parameters-of-knn">Parameters of kNN</a></li>
<li class="toctree-l4"><a class="reference internal" href="#linear-regression">Linear Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pros-and-cons-of-ls-for-classification">Pros and Cons of LS for Classification</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-complexity-of-knn-and-linear-regression">Model Complexity of kNN and Linear Regression</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="w1_2_simulation_study.html">1.2.2. Simulation Study</a></li>
<li class="toctree-l3"><a class="reference internal" href="w1_2_bayes_rule.html">1.2.3. Compute Bayes rule</a></li>
<li class="toctree-l3"><a class="reference internal" href="w1_2_discussion.html">1.2.4. Discussion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w4/w4_index.html">4. Regression Trees and Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w5/w5_index.html">5. Nonlinear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w6/w6_index.html">6. Clustering Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w7/w7_index.html">7. Latent Structure Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w9/w9_index.html">9. Discriminant Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w11/w11_index.html">11. Support Vector Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w1_index.html"><span class="section-number">1. </span>Introduction</a></li>
          <li class="breadcrumb-item"><a href="w1_2_index.html"><span class="section-number">1.2. </span>Least squares vs. nearest neighbors</a></li>
      <li class="breadcrumb-item active"><span class="section-number">1.2.1. </span>Introduction to LS and kNN</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/w1/w1_2_intro_LS_kNN.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="introduction-to-ls-and-knn">
<h1><span class="section-number">1.2.1. </span>Introduction to LS and kNN<a class="headerlink" href="#introduction-to-ls-and-knn" title="Link to this heading"></a></h1>
<p>In this section, we will examine two simple but popular supervised learning approaches: k-Nearest Neighbor (kNN) and linear regression. We’ll evaluate their performance using two simulated toy examples to illuminate the trade-off between bias and variance.</p>
<section id="k-nearest-neighbor-knn">
<h2>k-Nearest Neighbor (kNN)<a class="headerlink" href="#k-nearest-neighbor-knn" title="Link to this heading"></a></h2>
<p>The k-Nearest Neighbor (kNN) algorithm is a remarkably simple approach that requires no training. When presented with a test point x, we identify the top k samples in the training data that are closest (in terms of x) to this test point. For regression tasks, we simply output the average of the Y values corresponding to these top k neighbors. For classification, we can either return the majority vote or a probability calculated based on the class frequency within this neighborhood.</p>
<p>For example, consider a classification problem with k = 5. Among the top five neighbors for a specific test point, if three have a Y value of 1 and the other two have a Y value of 0, majority voting predicts Y as 1 because there are more Y=1 data points in this neighborhood. Alternatively, if we return a probability vector, the probability of Y=1 for this test point would be 3 out of 5, and the probability of Y=0 would be 2 out of 5.</p>
</section>
<section id="parameters-of-knn">
<h2>Parameters of kNN<a class="headerlink" href="#parameters-of-knn" title="Link to this heading"></a></h2>
<p>One key input parameter is k, the neighborhood size. For 1NN (k=1), the nearest training sample to the test point is used for prediction. This results in a training error of zero, as the prediction at the ith training sample equals the corresponding Y value of ith training sample. The upper limit for k is the sample size (n). When k equals n, the prediction remains constant for all x values, akin to fitting a model with only an intercept. Apparently, the flexibility of kNN is inversely related to k. No magic value for k. It’s commonly treated as a training parameter, often determined through cross-validation.</p>
<p>The other input parameter is the metric that defines the neighborhood. The default choice is Euclidean distance for p-dimensional feature vectors X, but it can be customized, such as using weighted Euclidean distance where dimension weights can be adjusted to influence their impact. It does not need to be Euclidean, as long as it is a similarity measure for any two samples. For instance, in image classification (from Flickr), we can measure the similarity of two images by their pixel similarity, or by the similarity of their tags, or by the percentage of people who like both images.</p>
</section>
<section id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading"></a></h2>
<p>Linear regression is another simple supervised learning algorithm.  In this model, the response variable Y is approximated using a linear function of X. If X is p-dimensional, we estimate ‘p+1’ parameters, including the intercept. Least squares that minimizes the quadratic loss is commonly employed for estimating these parameters due to its computation efficiency.</p>
<p>For binary classification, we code the binary response as 0 or 1. Predictions are formed by thresholding at 0.5. Though the cutoff value can be tuned for better performance, we’ll use the default 0.5 in our simulation study.</p>
</section>
<section id="pros-and-cons-of-ls-for-classification">
<h2>Pros and Cons of LS for Classification<a class="headerlink" href="#pros-and-cons-of-ls-for-classification" title="Link to this heading"></a></h2>
<p>There are drawbacks to using least squares for classification. The squared differences between binary outcomes and numerical predictions fail to effectively gauge classification performance. For example, a numerical prediction of 2 and a numerical prediction of 0 for a point with label Y=1 yield equal squared errors, but the former is a correct prediction and the latter is not.</p>
<p>Furthermore, obtaining probability estimates is challenging, as linear regression can generate values outside the [0, 1] range required for probabilities.</p>
<p>Despite its drawbacks, the least square approach for classification performs well in practice and has fast computation. We’ll apply it to our two toy examples for benchmarking purposes.</p>
</section>
<section id="model-complexity-of-knn-and-linear-regression">
<h2>Model Complexity of kNN and Linear Regression<a class="headerlink" href="#model-complexity-of-knn-and-linear-regression" title="Link to this heading"></a></h2>
<p>When discussing model complexity,  we often refer to the number of parameters or the effective number of degrees of freedom (DF) in the model.  For parametric models like linear regression, this is straightforward: it’s the number of coefficients, p+1, if the feature vector X is p-dimensional.</p>
<p>But kNN is a non-parametric method, so things are a bit different. The complexity of kNN is inversely related to k and can be approximated as the ratio n/k. This relationship is evident when considering the following two extremes.</p>
<ol class="arabic simple">
<li><p>k=1: the training error is zero, and model complexity is at its highest. It behaves as if it has n distinct parameters. Each of the n training points can uniquely influence the prediction. For classification, this results in a jagged decision boundary, while in regression, if all y_i values differ in the training set, there would be n unique predictions.</p></li>
<li><p>k=n: The model complexity is minimal. It behaves like a model with one parameter. Regardless of the input, the model outputs the same prediction for all queries (for classification, it would be the majority class in the dataset; for regression, it would be the mean of the outputs).</p></li>
</ol>
<p><strong>Degree of Freedom (DF)</strong>:</p>
<p>Students often encounter the term DF in introductory statistics courses, related to t-tests and F-tests. For example, the DF of t-tests is n - (p+1) in a linear regression model with an intercept and a p dimensional feature X. This seems to contradict to what’s said earlier that the DF of such a linear models is (p+1).</p>
<p>However, the principle is consistent: in the context of linear regression,  DF always denotes the dimension of a relevant vector space. As we’ll learn next week, the LS prediction (of a model with p covariates), y-hat, lies is a (p+1) dimensional subpace,  while the residual vector, (y - y-hat), lies in a (n-p-1) dimensional subspace. The DF of the t-test refers to the DF of the residual vector since the residuals are used to compute the standard deviation used in the t-test, while the DF of the linear model measures the DF of the LS prediction y-hat.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w1_2_index.html" class="btn btn-neutral float-left" title="1.2. Least squares vs. nearest neighbors" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w1_2_simulation_study.html" class="btn btn-neutral float-right" title="1.2.2. Simulation Study" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>