<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>7.3. The EM Algorithm &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7.4. Latent Dirichlet Allocation Model" href="w7_4_LDA.html" />
    <link rel="prev" title="7.2. Mixture Models" href="w7_2_mixture.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../w1/w1_index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w4/w4_index.html">4. Regression Trees and Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w5/w5_index.html">5. Nonlinear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w6/w6_index.html">6. Clustering Analysis</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="w7_index.html">7. Latent Structure Models</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="w7_1_intro.html">7.1. Model-based Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="w7_2_mixture.html">7.2. Mixture Models</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">7.3. The EM Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">7.3.1. The EM Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="#why-it-works">7.3.2. Why It Works</a></li>
<li class="toctree-l3"><a class="reference internal" href="#connection-with-k-means">7.3.3. Connection with K-means</a></li>
<li class="toctree-l3"><a class="reference internal" href="#an-alternative-view-of-em">7.3.4. An Alternative View of EM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#variational-em">7.3.5. Variational EM</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="w7_4_LDA.html">7.4. Latent Dirichlet Allocation Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="w7_5_hmm.html">7.5. Hidden Markov Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w9/w9_index.html">9. Discriminant Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w11/w11_index.html">11. Support Vector Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w7_index.html"><span class="section-number">7. </span>Latent Structure Models</a></li>
      <li class="breadcrumb-item active"><span class="section-number">7.3. </span>The EM Algorithm</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="the-em-algorithm">
<h1><span class="section-number">7.3. </span>The EM Algorithm<a class="headerlink" href="#the-em-algorithm" title="Link to this heading"></a></h1>
<section id="id1">
<h2><span class="section-number">7.3.1. </span>The EM Algorithm<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<p>The Expectation-Maximization (EM) algorithm is an iterative technique designed to compute the Maximum Likelihood Estimation (MLE) by augmenting the sample data with unobserved latent variables.  For instance, in the context of a two-component Gaussian distribution, while we observe x_i, the algorithm suggests an iterative method incorporating a latent variable Z_i that denotes which component x_i originates from. It’s essential to note that Z_i itself isn’t observed; we exclusively observe x_i.</p>
<p>Let’s denote our data as <span class="math notranslate nohighlight">\(\mathbf{x}=(x_1, ... x_n)\)</span> with a corresponding log-likelihood expressed by a function of unknown parameters <span class="math notranslate nohighlight">\(\theta\)</span>.  Utilizing the latent variables <span class="math notranslate nohighlight">\(\mathbf{Z} = (Z_1, ..., Z_n)\)</span>, the log-likelihood can be expressed as the logarithm of a summation. Specifically, in the case of the two-component Gaussian distribution, this sum iterates over all z_i either taking the value one or two. A direct maximization of this function proves challenging due to the presence of the sum within the logarithm.</p>
<div class="math notranslate nohighlight">
\[\log p(\mathbf{x} | \theta) = \log \sum_{\mathbf{z}} p(\mathbf{x}, \mathbf{z}| \theta) = \log \sum_{\mathbf{z}} p(\mathbf{z} | \theta) p(\mathbf{x} | \mathbf{z} , \theta).\]</div>
<p>The EM algorithm circumvents this difficulty, by operating under the assumption that if <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> were known and subsequently working with the log of the joint likelihood:</p>
<div class="math notranslate nohighlight">
\[\log p(\mathbf{x}, \mathbf{Z} | \theta) = \log p(\mathbf{Z} | \theta) + \log p(\mathbf{x} | \mathbf{Z}, \theta).\]</div>
<p>Throughout the derivations, I consistently utilize uppercase Z to emphasize that it is a random variable, given that it’s not directly observed.</p>
<p>The EM algorithm consists of two iterative steps.</p>
<a class="reference internal image-reference" href="../_images/w7_EM_alg.png"><img alt="../_images/w7_EM_alg.png" src="../_images/w7_EM_alg.png" style="width: 542.25px; height: 288.0px;" /></a>
<p>The first, known as the E-step, involves taking an expectation. While our aim is to work with the log of the joint likelihood, the absence of observed Z values compels us to begin with an initial parameter value, <span class="math notranslate nohighlight">\(\theta_0\)</span>, and compute the expectation of Z given both the data and the initial <span class="math notranslate nohighlight">\(\theta_0\)</span>. In scenarios like the two-component Gaussian distribution, the conditional distribution of <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\theta_0\)</span> can be factored and computed independently for each Z_i. The expectation can be computed as a sum or integral, resulting in a function g.</p>
<p>Our next task is to find a <span class="math notranslate nohighlight">\(\theta_1\)</span> value that
maximizes g, subsequently replacing <span class="math notranslate nohighlight">\(\theta_0\)</span> with <span class="math notranslate nohighlight">\(\theta_1\)</span>. This iterative process, alternating between the E and M steps, continues until convergence is achieved.</p>
<p>The heuristic iterative approach we’ve proposed for the two-component normal essentially mirrors the EM (Expectation-Maximization) algorithm.</p>
</section>
<section id="why-it-works">
<h2><span class="section-number">7.3.2. </span>Why It Works<a class="headerlink" href="#why-it-works" title="Link to this heading"></a></h2>
<p>One essential proof remains: After each iteration, the EM algorithm improves the marginal likelihood of X. This means that for every iteration, the new parameter value <span class="math notranslate nohighlight">\(\theta_1\)</span> should yield a larger marginal likelihood than the previous value <span class="math notranslate nohighlight">\(\theta_0\)</span>.</p>
<p>To establish this, start by recalling the definition of <span class="math notranslate nohighlight">\(g(\theta)\)</span>, which is the expected value of the logarithm of the joint likelihood of <span class="math notranslate nohighlight">\((\mathbf{x}, \mathbf{Z})\)</span>, where expectation is calculated with respect to <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, conditional upon data <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\theta_0\)</span>. Note that regardless of the <span class="math notranslate nohighlight">\(\theta\)</span> value in <span class="math notranslate nohighlight">\(g(\theta)\)</span>, expectations are consistently taken with respect to <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\theta_0\)</span>.</p>
<div class="math notranslate nohighlight">
\[g(\theta) = \mathbb{E}_{\mathbf{Z} | \mathbf{x}, \theta_0} \log p(\mathbf{x}, \mathbf{Z} | \theta)\]</div>
<p>Now consider <span class="math notranslate nohighlight">\(g(\theta_1) - g(\theta_0).\)</span> Since the expectation remains consistent, this difference can be expressed as the logarithm of a ratio with <span class="math notranslate nohighlight">\(\theta_1\)</span> in the numerator and <span class="math notranslate nohighlight">\(\theta_0\)</span> in the denominator. Next, factor the joint likelihood of <span class="math notranslate nohighlight">\((\mathbf{x}, \mathbf{Z})\)</span> into the marginal likelihood of the marginal likelihood of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and the conditional likelihood of <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and whatever <span class="math notranslate nohighlight">\(\theta\)</span> value.</p>
<div class="math notranslate nohighlight">
\[\begin{split}g(\theta_1) - g(\theta_0) &amp;= \mathbb{E}_ {\mathbf{Z} | \mathbf{x}, \theta_0} \log \frac{p(\mathbf{x}, \mathbf{Z} | \theta_1)}{p(\mathbf{x}, \mathbf{Z} | \theta_0)}  = \mathbb{E}_ {\mathbf{Z} | \mathbf{x}, \theta_0} \log \frac{p(\mathbf{x} | \theta_1) p (\mathbf{Z} | \mathbf{x}, \theta_1)}{p(\mathbf{x} | \theta_0) p(\mathbf{Z} | \mathbf{x}, \theta_0)}  \\
&amp;= \log \frac{p(\mathbf{x} | \theta_1)}{p(\mathbf{x} | \theta_0)} - \mathbb{E}_ {\mathbf{Z} | \mathbf{x}, \theta_0} \log \frac{p(\mathbf{Z} | \mathbf{x}, \theta_0)}{p( \mathbf{Z} | \mathbf{x}, \theta_1)}\end{split}\]</div>
<p>We arrive at the final result by decomposing the terms inside the logarithm into two separate ratios. The ratio to the left, independent of Z, can be pulled outside of the expectation, while the ratio to the right is inverted and negated.</p>
<p>Rearranging these terms, we reach a specific expression:</p>
<div class="math notranslate nohighlight">
\[\log \frac{p(\mathbf{x}| \theta_1)}{p(\mathbf{x} | \theta_0)}  = \underbrace{g(\theta_1) - g(\theta_0)}_{\ge 0} +  \underbrace{\mathbb{E}_ {\mathbf{Z} | \mathbf{x}, \theta_0} \log \frac{p(\mathbf{Z} | \mathbf{x}, \theta_0)}{p( \mathbf{Z} | \mathbf{x}, \theta_1)}}_{\ge 0}.\]</div>
<p>We can infer that the left-hand side is non-negative, since 1) the non-negativity of the KL-divergence between the two distributions over Z; and 2) our algorithm ensures that the difference <span class="math notranslate nohighlight">\(g(\theta_1) - g(\theta_0)\)</span> remains non-negative. Hence, we confirm that the iterative process indeed improves the marginal likelihood with each step.</p>
</section>
<section id="connection-with-k-means">
<h2><span class="section-number">7.3.3. </span>Connection with K-means<a class="headerlink" href="#connection-with-k-means" title="Link to this heading"></a></h2>
<p>The EM algorithm for Gaussian mixtures bears a resemblance to the K-means algorithm. Let’s dive deeper into their similarities and differences.</p>
<ul class="simple">
<li><dl class="simple">
<dt>In the EM algorithm:</dt><dd><p>The <span class="math notranslate nohighlight">\(\gamma_i\)</span>’s are probabilities that lie between zero and one.’</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>In the K-means algorithm:</dt><dd><p>The <span class="math notranslate nohighlight">\(\gamma_i\)</span>’s take on only two values: one or zero. This means each observation is assigned to the single cluster it is closest to.</p>
</dd>
</dl>
</li>
</ul>
<p>A natural question that arises is: under what circumstances would the EM algorithm mimic the k-means algorithm? In other words, when would our <span class="math notranslate nohighlight">\(\gamma_i\)</span>’s be close to either one or zero?</p>
<p>To explore this, let’s consider the ratio <span class="math notranslate nohighlight">\(\gamma_i/(1- \gamma_i)\)</span>. For the sake of simplicity, let’s make an assumption that the variance of two normals are identical. Under this assumption, our ratio can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\frac{\pi}{1-\pi} \times \exp \Big ( -\frac{1}{\sigma^2}\big [ (x_i - \mu_1)^2 - (x_i - \mu_2)^2 \big ] \Big ).\]</div>
<p>Now, let’s think of a scenario where <span class="math notranslate nohighlight">\(\sigma^2\)</span> is very small, close to one. This implies that  <span class="math notranslate nohighlight">\(\frac{1}{\sigma^2}\)</span> is very large. If <span class="math notranslate nohighlight">\(x_i\)</span> is just marginally closer to <span class="math notranslate nohighlight">\(\mu_1\)</span>, the resulting product inside the exponential function is a large negative number, making the ratio <span class="math notranslate nohighlight">\(\gamma_i/(1- \gamma_i)\)</span> very large. Consequently, <span class="math notranslate nohighlight">\(\gamma_i\)</span> approaches one. By the same logic, if <span class="math notranslate nohighlight">\(x_i\)</span> is slightly closer to <span class="math notranslate nohighlight">\(\mu_2\)</span>, <span class="math notranslate nohighlight">\(\gamma_i\)</span> nears zero.</p>
<p>In summary, the K-means algorithm can be perceived as a special case of the EM algorithm for Gaussian mixtures, where the error variance approaches zero.</p>
</section>
<section id="an-alternative-view-of-em">
<h2><span class="section-number">7.3.4. </span>An Alternative View of EM<a class="headerlink" href="#an-alternative-view-of-em" title="Link to this heading"></a></h2>
<p>Let’s explore an alternative perspective on the EM algorithm, drawing from a paper by Neal and Hinton in 1998.</p>
<p>Envision the following objective function where the numerator represents the joint likelihood of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> given <span class="math notranslate nohighlight">\(\theta\)</span> and the denominator is a distribution on <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> – a pdf for continuous values or a probability mass function for discrete ones. On the right-hand side, we have our known data <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, with <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> being integrated over (since we’re taking the external expectation with respect to q). Both <span class="math notranslate nohighlight">\(\theta\)</span> and the distribution q are unknown, making this a function of these two variables.</p>
<div class="math notranslate nohighlight">
\[F(q, \theta) = \mathbb{E}_{q(\mathbf{Z})} \log \frac{p(\mathbf{x}, \mathbf{Z} | \theta)}{q(\mathbf{Z})}.\]</div>
<p>To manipulate this objective function, we’ll employ a familiar technique. We’ll factorize the numerator as a combination of the marginal of x and the conditional of Z given x. Since the marginal of x is independent of Z, we can extract this term from the  expectation.</p>
<div class="math notranslate nohighlight">
\[\begin{split}F(q, \theta) &amp; = \mathbb{E}_{q(\mathbf{Z})} \log \frac{p(\mathbf{x} | \theta) p(\mathbf{Z}| \mathbf{x}, \theta)}{q(\mathbf{Z})} \\
&amp; =  \log p(\mathbf{x} | \theta) - \mathbb{E}_{q(\mathbf{Z})} \log \frac{q(\mathbf{Z})}{p(\mathbf{Z} | \mathbf{x}, \theta)}\end{split}\]</div>
<p>This process divides our function into two segments: The first is the logarithm of the marginal of x, and the second, a negative KL divergence between two distributions over Z. To optimize F, we should maximize each segment individually. For the first term, we can attain its peak value by setting <span class="math notranslate nohighlight">\(\theta\)</span> to its Maximum Likelihood Estimate (MLE), thereby maximizing <span class="math notranslate nohighlight">\(\log p(\mathbf{x} | \theta)\)</span>. For the second term, the goal is to minimize the KL divergence, ideally to zero, achieved by setting <span class="math notranslate nohighlight">\(q(\mathbf{z}) = p(\mathbf{z} | \mathbf{x}, \hat{\theta}_{\text{mle}})\)</span>. Essentially, we can deduce the MLE of <span class="math notranslate nohighlight">\(\theta\)</span> by optimizing this function, F.</p>
<p>Now, how do we go about optimizing F? It becomes evident that the EM algorithm operates like a coordinate ascent on F. At each t-th iteration, given the current value of <span class="math notranslate nohighlight">\(\theta^t\)</span>, we determine the optimal q. As discussed, the ideal q minimizes the KL divergence, leading to <span class="math notranslate nohighlight">\(q(\mathbf{z}) = p(\mathbf{z} | \mathbf{x}, \theta^t)\)</span>. Subsequently, using the optimal q, we update <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<a class="reference internal image-reference" href="../_images/w7_EM_alternative.png"><img alt="../_images/w7_EM_alternative.png" src="../_images/w7_EM_alternative.png" style="width: 558.0px; height: 96.75px;" /></a>
<p>From this perspective, the EM algorithm’s E-step and M-step are indistinguishable. They’re both essentially M-steps, iteratively optimizing function F. This viewpoint offers a rationale for certain EM algorithm variants that may only partially implement the E or M steps, provided the ensuing q or <span class="math notranslate nohighlight">\(\theta\)</span> values improve the function F.</p>
<p>Furthermore, in some applications, constraints are imposed on the q function, as highlighted in <a class="reference external" href="https://papers.nips.cc/paper_files/paper/2007/hash/73e5080f0f3804cb9cf470a8ce895dac-Abstract.html">Ganchev et al. (2007)</a>. Given the E-step can be seen as an M-step in this alternative view, integrating these constraints becomes more straightforward.</p>
<p>A significant advantage of this approach is its seamless transition to the variational version of the EM algorithm.</p>
</section>
<section id="variational-em">
<h2><span class="section-number">7.3.5. </span>Variational EM<a class="headerlink" href="#variational-em" title="Link to this heading"></a></h2>
<p>In the conventional EM algorithm, given a parameter <span class="math notranslate nohighlight">\(\theta\)</span>, the goal is to determine q that maximizes F by setting <span class="math notranslate nohighlight">\(q(\mathbf{z})\)</span> to exactly match <span class="math notranslate nohighlight">\(p(\mathbf{z} | \mathbf{x}, \theta^t)\)</span>. However, this function might be computationally challenging to evaluate.</p>
<p>To make computations more manageable, we introduce an approximation to this function. For instance, <span class="math notranslate nohighlight">\(q(\mathbf{z})\)</span> might be represented in a factorized manner, decomposing it into n different q functions, each corresponding to an individual latent variable Z_i. Using this form of q, our objective function now depends on <span class="math notranslate nohighlight">\(\theta\)</span> and a series of q’s ranging from q_1 to q_n:</p>
<div class="math notranslate nohighlight">
\[F(\theta, q_1, \cdots, q_n) = \mathbb{E}_{q_1, \dots, q_n} \log \frac{p(\mathbf{x} | \theta) p(\mathbf{Z} | \mathbf{x}, \theta)}{q_1(Z_1) \cdots q_n(Z_n)}.\]</div>
<p>Employing coordinate descent, we iteratively optimize <span class="math notranslate nohighlight">\(\theta\)</span> and each q. In many machine learning and statistical contexts, the updates in each iteration of the coordinate descent/ascent are straightforward, often possessing closed-form solutions. This ensures rapid algorithm convergence.</p>
<p>In the following section, I will introduce a more intricate mixture model than previously discussed. Due to its complexity, its parameters necessitate estimation through the variational EM, rather than the standard EM.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w7_2_mixture.html" class="btn btn-neutral float-left" title="7.2. Mixture Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w7_4_LDA.html" class="btn btn-neutral float-right" title="7.4. Latent Dirichlet Allocation Model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>