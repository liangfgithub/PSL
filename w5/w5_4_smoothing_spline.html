<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>5.4. Smoothing Splines &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5.5. Local Regression" href="w5_5_local_reg.html" />
    <link rel="prev" title="5.3. Regression Splines" href="w5_3_reg_spline.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../w1/w1_index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w4/w4_index.html">4. Regression Trees and Ensemble</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="w5_index.html">5. Nonlinear Regression</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="w5_1_poly.html">5.1. Polynomial Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="w5_2_spline.html">5.2. Cubic Splines</a></li>
<li class="toctree-l2"><a class="reference internal" href="w5_3_reg_spline.html">5.3. Regression Splines</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">5.4. Smoothing Splines</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">5.4.1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#the-roughness-penalty-approach">5.4.2. The Roughness Penalty Approach</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#a-fundamental-result">A Fundamental Result</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#fit-a-smoothing-spline-model">5.4.3. Fit a Smoothing Spline Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#the-dr-basis">The DR Basis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#the-effective-degrees-of-freedom">The Effective Degrees of Freedom</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#smoothing-splines-in-r-python">5.4.4. Smoothing Splines in R/Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="#choice-of-lambda">5.4.5. Choice of Lambda</a></li>
<li class="toctree-l3"><a class="reference internal" href="#weighted-spline-models">5.4.6. Weighted Spline Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#summary">5.4.7. Summary</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="w5_5_local_reg.html">5.5. Local Regression</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w6/w6_index.html">6. Clustering Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w7/w7_index.html">7. Latent Structure Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w9/w9_index.html">9. Discriminant Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w11/w11_index.html">11. Support Vector Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w5_index.html"><span class="section-number">5. </span>Nonlinear Regression</a></li>
      <li class="breadcrumb-item active"><span class="section-number">5.4. </span>Smoothing Splines</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="smoothing-splines">
<h1><span class="section-number">5.4. </span>Smoothing Splines<a class="headerlink" href="#smoothing-splines" title="Link to this heading"></a></h1>
<section id="introduction">
<h2><span class="section-number">5.4.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>In regression splines, we have to decide on the number of knots and their locations. This choice often has significant implications for the resulting fit.</p>
<p>As a solution to the arbitrariness of knot placement in regression splines, <strong>smoothing splines</strong> take a naive approach. Knots are placed at every data point, simplifying the decision-making process. For a natural cubic spline, when knots are placed at n data points, our design matrix becomes of size n-by-n.</p>
<div class="math notranslate nohighlight">
\[\mathbf{y}_{n \times 1} = \mathbf{F}_{n \times n} \boldsymbol{\beta}_{n \times 1}.\]</div>
<p>Running ordinary least squares (OLS) regression with this matrix would yield a perfect fit to the data. To avoid overfitting, one can employ techniques like Lasso. Smoothing splines employ an approach similar to ridge regression.</p>
<div class="math notranslate nohighlight">
\[\min_{\boldsymbol{\beta}} \Big [ \| \mathbf{y} - \mathbf{F} \boldsymbol{\beta} \|^2 + \textcolor{red}{\lambda} \boldsymbol{\beta}^t \Omega \boldsymbol{\beta} \Big ].\]</div>
<p>The objective function has two components:</p>
<ul class="simple">
<li><p>Residual Sum of Squares (RSS), which captures the fit of the model.</p></li>
<li><p>A quadratic penalty on beta, which discourages large coefficient values and thus reduces overfitting.</p></li>
</ul>
<p>Specifically, the penalty is quadratic, resembling the penalty in ridge regression. In typical ridge regression,  <span class="math notranslate nohighlight">\(\Omega\)</span> is an identity matrix, meaning each parameter is penalized equally. However, in the context of smoothing splines, <span class="math notranslate nohighlight">\(\Omega\)</span> might not be the identity, and the nature of this matrix will be defined later.</p>
<p>The tuning parameter <span class="math notranslate nohighlight">\(\lambda\)</span>, often chosen by CV, decides the balance between the fit and the penalty. A larger <span class="math notranslate nohighlight">\(\lambda\)</span> places more emphasis on regularization and can lead to smoother curves, while a smaller <span class="math notranslate nohighlight">\(\lambda\)</span> allows for more flexibility in fitting the data.</p>
<p>Next, we’re going to see that how this objective function for smoothing splines can be derived from a different aspect.</p>
</section>
<section id="the-roughness-penalty-approach">
<h2><span class="section-number">5.4.2. </span>The Roughness Penalty Approach<a class="headerlink" href="#the-roughness-penalty-approach" title="Link to this heading"></a></h2>
<p>Assume the function we aim to estimate has a bounded support [a, b]. Let <span class="math notranslate nohighlight">\(S[a,b]\)</span> denote all smooth functions defined on [a,b]. Instead of merely trying to fit the data as closely as possible, let’s look for the minimizer in <span class="math notranslate nohighlight">\(S[a,b]\)</span> of the following penalized residual sum of squares</p>
<div class="math notranslate nohighlight">
\[\text{RSS}_ \lambda(g) = \sum_{i=1}^n  [y_i - g(x_i)]^2 + \lambda \int_a^b [g{''}(x) ]^2 d x,\]</div>
<p>The first term ensures that <span class="math notranslate nohighlight">\(g\)</span> fits the given data well.</p>
<p>The second term is a roughness penalty on function <span class="math notranslate nohighlight">\(g\)</span>. The second derivative is a measure of the curvature or “wiggliness” of a function. By squaring it and integrating over the entire range, we obtain a measure of how much the function fluctuates. For instance, if <span class="math notranslate nohighlight">\(g\)</span> linear, its second derivative is zero throughout the range, resulting in no penalty. Functions with a lot of wiggles or changes in curvature will have a large penalty due to high values of the second derivative.</p>
<p>The parameter <span class="math notranslate nohighlight">\(\lambda\)</span> controls the trade-off. A larger <span class="math notranslate nohighlight">\(\lambda\)</span> means we prefer smoother functions, even if they fit the data a bit worse. A small <span class="math notranslate nohighlight">\(\lambda\)</span> allows for more wigglly functions if they fit the data better.</p>
<p>The amazing part is that even though this optimization problem presented in an infinite-dimensional function space <span class="math notranslate nohighlight">\(S[a,b]\)</span>, it can be solved using finite-dimensional linear algebra, which brings us back to the ridge regression problem you presented earlier.</p>
<section id="a-fundamental-result">
<h3>A Fundamental Result<a class="headerlink" href="#a-fundamental-result" title="Link to this heading"></a></h3>
<p>The best smooth function <span class="math notranslate nohighlight">\(g\)</span> that minimizes the penalized residual sum of squares over the entire infinite-dimensional function space <span class="math notranslate nohighlight">\(S[a,b]\)</span> happens to be a natural cubic spline (NCS) with knots at the data points x_1, …, x_n, where we assume the n data points are unique. We’ll discuss how to handle the non-unique data points later.</p>
<p><strong>Theorem</strong>. (WLOG) Assume n&gt;1 and the data points are unique and arranged in an increasing order: x_1 &lt; x_2 &lt; … &lt; x_n. The following holds true:</p>
<div class="math notranslate nohighlight">
\[\min_{g \in S[a,b]} \text{RSS}_\lambda(g) = \min_{\tilde{g} \in \text{NCS}_n} \text{RSS}_ \lambda(\tilde{g})\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{NCS}_n\)</span> refers to the family of natural cubic splines with knots at the n data points x_1, …, x_n.</p>
<p>How to prove it? Let’s consider any <span class="math notranslate nohighlight">\(g \in S[a,b]\)</span>. Then there exists an NCS <span class="math notranslate nohighlight">\(\tilde{g}\)</span> with knots at x_1, …, x_n satifying</p>
<div class="math notranslate nohighlight">
\[g(x_i) = \tilde{g}(x_i), \quad i=1, \dots, n.\]</div>
<p>Note that such a function <span class="math notranslate nohighlight">\(\tilde{g}\)</span> must indeed exist. This is due to the fact that a Natural Cubic Spline (NCS) with n knots possesses n degrees of freedom. Consequently, we can aptly choose the n coefficients to ensure the n predictions <span class="math notranslate nohighlight">\(\tilde{g}(x_i)\)</span> match any predetermined values.</p>
<a class="reference internal image-reference" href="../_images/w5_ss_proof_1.png"><img alt="../_images/w5_ss_proof_1.png" src="../_images/w5_ss_proof_1.png" style="width: 617.6px; height: 411.20000000000005px;" /></a>
<a class="reference internal image-reference" href="../_images/w5_ss_proof_2.png"><img alt="../_images/w5_ss_proof_2.png" src="../_images/w5_ss_proof_2.png" style="width: 620.0px; height: 438.40000000000003px;" /></a>
</section>
</section>
<section id="fit-a-smoothing-spline-model">
<h2><span class="section-number">5.4.3. </span>Fit a Smoothing Spline Model<a class="headerlink" href="#fit-a-smoothing-spline-model" title="Link to this heading"></a></h2>
<p>This result drastically simplifies the problem. Instead of looking over <span class="math notranslate nohighlight">\(S[a,b]\)</span>, an infinite space of functions, we can restrict our search to a much smaller, finite-dimensional space <span class="math notranslate nohighlight">\(\text{NCS}_n\)</span></p>
<p>Given that <span class="math notranslate nohighlight">\(g\)</span> is a natural cubic spline with n degrees of freedom, it can be represented as a linear combination of n basis functions:</p>
<div class="math notranslate nohighlight">
\[g(x) = \sum_{i=1}^n \beta_i h_i(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(h_i(x)\)</span> are basis functions for NBS with knots at x_1, …, x_n. With the parameterization in place, our original infinite-dimensional optimization problem is now reduced to a finite-dimensional one. Specifically, we’re now tasked with finding the best values of the coefficients <span class="math notranslate nohighlight">\(\beta_1, \dots, \beta_n\)</span>.</p>
<p>When you plug the expression for <span class="math notranslate nohighlight">\(g(x)\)</span> into the penalized objective function <span class="math notranslate nohighlight">\(\text{RSS}_ \lambda(g)\)</span>, you end up with a quadratic function in the beta’s. This can be minimized using standard linear algebra techniques.</p>
<p>Moreover, when this is represented in matrix form, it mirrors the ridge regression problem we discussed earlier. The design matrix <span class="math notranslate nohighlight">\(\mathbf{F}\)</span> contains the evaluations of the spline basis functions at the data points, and the penalty matrix <span class="math notranslate nohighlight">\(\Omega\)</span> captures the roughness of the spline, coming from the integral of the squared second derivatives.</p>
<a class="reference internal image-reference" href="../_images/w5_ss_matrix_rep_1.png"><img alt="../_images/w5_ss_matrix_rep_1.png" src="../_images/w5_ss_matrix_rep_1.png" style="width: 541.6px; height: 279.2px;" /></a>
<a class="reference internal image-reference" href="../_images/w5_ss_matrix_rep_2.png"><img alt="../_images/w5_ss_matrix_rep_2.png" src="../_images/w5_ss_matrix_rep_2.png" style="width: 493.6px; height: 277.6px;" /></a>
<p>If we were to employ a different set of basis functions, say <span class="math notranslate nohighlight">\(\tilde{h}_1(x), \dots, \tilde{h}_n(x)\)</span>, how might that affect our calculations?</p>
<p>Given a different set of basis functions, let’d note the associated design matrix by <span class="math notranslate nohighlight">\(\tilde{\mathbf{F}}\)</span> and the coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span>. Though <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> may differ due to the change in the design matrix, the resulting prediction <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span> should be the same.</p>
<p>This stems from the fact that if <span class="math notranslate nohighlight">\(h_i(x)\)</span> and <span class="math notranslate nohighlight">\(\tilde{h}_i(x)\)</span> represent distinct sets basis functions for an identical functional space, they can be interchanged through a full rank n-by-n matrix <span class="math notranslate nohighlight">\(A\)</span>. Then we have</p>
<div class="math notranslate nohighlight">
\[\tilde{\mathbf{F}}  = \mathbf{F} A, \quad \tilde{\Omega} = A^t \Omega A\]</div>
<p>When we substitute this expression into the experssion for the smoother matrix <span class="math notranslate nohighlight">\(\tilde{\mathbf{F}} (\tilde{\mathbf{F}}^t\tilde{\mathbf{F}} + \lambda  \tilde{\Omega})^{-1} \tilde{\mathbf{F}}^t\)</span>, terms with matrix A cancel out. This implies that our smoother matrix remains unchanged irrespective of whether we use <span class="math notranslate nohighlight">\(\mathbf{F}\)</span> or <span class="math notranslate nohighlight">\(\tilde{\mathbf{F}}\)</span>. In essence, the choice of basis functions doesn’t alter the S matrix, and, as a result, has no impact on <span class="math notranslate nohighlight">\(\hat{\mathbf{y}}\)</span>.</p>
<section id="the-dr-basis">
<h3>The DR Basis<a class="headerlink" href="#the-dr-basis" title="Link to this heading"></a></h3>
<p>A particular set of basis functions, termed the “DR basis” (Demmler &amp; Reinsch, 1975), boasts a unique double orthogonality property. Here, the basis functions are orthogonal to each other, and so are their second derivatives. Consequently,</p>
<a class="reference internal image-reference" href="../_images/w5_ss_DR.png"><img alt="../_images/w5_ss_DR.png" src="../_images/w5_ss_DR.png" style="width: 570.4px; height: 335.20000000000005px;" /></a>
<p>To understand why the rank of <span class="math notranslate nohighlight">\(\Omega\)</span> is less than n-2 (or put another way, why <span class="math notranslate nohighlight">\(\Omega\)</span> has two zero eigenvalues), it’s useful to refer back to our earlier discussion about changing basis functions.</p>
<p>Even though the actual expression for <span class="math notranslate nohighlight">\(\Omega\)</span> may vary as we switch between different sets of basis functions, its rank remains invariant. This consistency stems from the fact that moving from one basis function set to another invariably entails the use of a full-rank transformation matrix <span class="math notranslate nohighlight">\(A\)</span>. Consequently, the rank persists.</p>
<p>Now, let’s examine a specific choice of basis function. Here, we define <span class="math notranslate nohighlight">\(h_1(x) = 1\)</span> and <span class="math notranslate nohighlight">\(h_2(x) = x\)</span>. These functions, being global linear functions, naturally fall under the category of natural cubic splines. For the purpose of our current discussion, we only need to focus on these two functions, without concerning ourselves with additional functions. With this basis set in consideration and given that the second derivatives of <span class="math notranslate nohighlight">\(h_1\)</span> and <span class="math notranslate nohighlight">\(h_2\)</span> are null, the first two rows and columns of <span class="math notranslate nohighlight">\(\Omega\)</span> are  entirely zero. Consequently, the rank of <span class="math notranslate nohighlight">\(\Omega\)</span> is always less than or equal to n-2.</p>
<p>Employing this basis, the smooth spline coefficients can be expressed in a distinct manner. A notable “shrinkage” effect becomes evident. Each element of the smooth spline coefficient represents a shrunk estimate of the original least square estimate. Notably, the first two coefficients see no shrinkage since <span class="math notranslate nohighlight">\(d_1\)</span> and <span class="math notranslate nohighlight">\(d_2\)</span> are both zero. This underscores a key facet of the smoothing spline: linear terms are not subjected to penalization.</p>
</section>
<section id="the-effective-degrees-of-freedom">
<h3>The Effective Degrees of Freedom<a class="headerlink" href="#the-effective-degrees-of-freedom" title="Link to this heading"></a></h3>
<p>The effective degree of freedom (DF) of a smoothing spline is similar to what’s defined for ridge regression. It’s the trace of the smoother matrix <span class="math notranslate nohighlight">\(\mathbf{S}_{\lambda}\)</span>.</p>
<a class="reference internal image-reference" href="../_images/w5_ss_DR_edf.png"><img alt="../_images/w5_ss_DR_edf.png" src="../_images/w5_ss_DR_edf.png" style="width: 594.4px; height: 228.0px;" /></a>
<p>Even if <span class="math notranslate nohighlight">\(\lambda\)</span> tends to infinity, the first two terms in this ratio are 1 (since <span class="math notranslate nohighlight">\(d_1 = d_2 = 0\)</span>. This means linear terms aren’t penalized. Thus, the range for the DF for a smooth spline is between 2 and n. Similar to ridge, the DF doesn’t have to be an integer; it can be any real number, like 3.5.</p>
<p>When using software like R, rather than specifying <span class="math notranslate nohighlight">\(\lambda\)</span> value directly, one can indicate the desired degree of freedom. The software then determines the associated <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
</section>
</section>
<section id="smoothing-splines-in-r-python">
<h2><span class="section-number">5.4.4. </span>Smoothing Splines in R/Python<a class="headerlink" href="#smoothing-splines-in-r-python" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Rcode: [<a class="reference external" href="https://liangfgithub.github.io/Rcode_W5_SmoothingSpline.html">Rcode_W5_SmoothingSpline</a>]</p></li>
<li><p>Python: [<a class="reference external" href="https://liangfgithub.github.io/Python_W5_SmoothingSpline.html">Python_W5_SmoothingSpline</a>]</p></li>
</ul>
</section>
<section id="choice-of-lambda">
<h2><span class="section-number">5.4.5. </span>Choice of Lambda<a class="headerlink" href="#choice-of-lambda" title="Link to this heading"></a></h2>
<p>Various methods exist to select lambda, such as cross-validation or using a separate test set for assessing prediction error. Many R packages have integrated functions to select lambda, often leveraging leave-one-out cross-validation (LOOCV) and generalized cross-validation (GCV).</p>
<a class="reference internal image-reference" href="../_images/w5_ss_lambda_selection.png"><img alt="../_images/w5_ss_lambda_selection.png" src="../_images/w5_ss_lambda_selection.png" style="width: 507.20000000000005px; height: 294.40000000000003px;" /></a>
<p>Leave-one-out cross-validation is defined as the average difference between <span class="math notranslate nohighlight">\(y_i\)</span> and <span class="math notranslate nohighlight">\(\hat{g}(x_i)^{[-i]}\)</span>, the predicted value for <span class="math notranslate nohighlight">\(x_i\)</span> when the data pair <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> is excluded from the model. Initially, this seems to necessitate n model fits, each excluding one observation. However, it’s more efficient in practice: we can compute the LOOCV prediction error based on the original model – the one fit using all data points. This formula isn’t exclusive to smoothing splines; it also applies to models like linear regression, polynomial regression, and ridge regression.</p>
<a class="reference internal image-reference" href="../_images/w5_ss_loocv.png"><img alt="../_images/w5_ss_loocv.png" src="../_images/w5_ss_loocv.png" style="width: 557.6px; height: 420.75px;" /></a>
<p>In some cases, GCV is used to lessen computational demands. Instead of using varying <span class="math notranslate nohighlight">\(S_{ii}\)</span> values (which are different for each i), we take their average, which serves as an approximation.</p>
</section>
<section id="weighted-spline-models">
<h2><span class="section-number">5.4.6. </span>Weighted Spline Models<a class="headerlink" href="#weighted-spline-models" title="Link to this heading"></a></h2>
<a class="reference internal image-reference" href="../_images/w5_weighted_ss.png"><img alt="../_images/w5_weighted_ss.png" src="../_images/w5_weighted_ss.png" style="width: 603.5px; height: 404.59999999999997px;" /></a>
</section>
<section id="summary">
<h2><span class="section-number">5.4.7. </span>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<a class="reference internal image-reference" href="../_images/w5_ss_summary.png"><img alt="../_images/w5_ss_summary.png" src="../_images/w5_ss_summary.png" style="width: 585.65px; height: 339.15px;" /></a>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w5_3_reg_spline.html" class="btn btn-neutral float-left" title="5.3. Regression Splines" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w5_5_local_reg.html" class="btn btn-neutral float-right" title="5.5. Local Regression" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>