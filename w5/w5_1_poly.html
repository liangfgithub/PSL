<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>5.1. Polynomial Regression &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5.2. Regression Splines" href="w5_2_reg_spline.html" />
    <link rel="prev" title="5. Nonlinear Regression" href="w5_index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../w1/w1_index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w4/w4_index.html">4. Regression Trees and Ensemble</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="w5_index.html">5. Nonlinear Regression</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">5.1. Polynomial Regression</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#fit-a-polynomial-regression-model">5.1.1. Fit a Polynomial Regression Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#choosing-the-degree">5.1.2. Choosing the Degree</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#questioning-the-selection-of-d">Questioning the Selection of <span class="math notranslate nohighlight">\(d\)</span></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#implementing-in-r-python">5.1.3. Implementing in R/Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="#global-vs-local">5.1.4. Global vs Local</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="w5_2_reg_spline.html">5.2. Regression Splines</a></li>
<li class="toctree-l2"><a class="reference internal" href="w5_3_smoothing_spline.html">5.3. Smoothing Splines</a></li>
<li class="toctree-l2"><a class="reference internal" href="w5_4_local_reg.html">5.4. Local Regression</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w6/w6_index.html">6. Clustering Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w7/w7_index.html">7. Latent Structure Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w9/w9_index.html">9. Discriminant Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w11/w11_index.html">11. Support Vector Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w5_index.html"><span class="section-number">5. </span>Nonlinear Regression</a></li>
      <li class="breadcrumb-item active"><span class="section-number">5.1. </span>Polynomial Regression</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="polynomial-regression">
<h1><span class="section-number">5.1. </span>Polynomial Regression<a class="headerlink" href="#polynomial-regression" title="Link to this heading"></a></h1>
<section id="fit-a-polynomial-regression-model">
<h2><span class="section-number">5.1.1. </span>Fit a Polynomial Regression Model<a class="headerlink" href="#fit-a-polynomial-regression-model" title="Link to this heading"></a></h2>
<p>Polynomial regression is a type of regression analysis used when the relationship between the independent variable x and the dependent variable y is non-linear. For this discussion, x is considered to be one-dimensional. Extensions to multi-dimensional cases will be covered later.</p>
<p>Polynomial regression can be seen as a specialized case of multiple linear regression. Consider a polynomial model of degree ‘d’:</p>
<div class="math notranslate nohighlight">
\[y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 \cdots + \beta_d x_i^d + \text{err}.\]</div>
<p>The coefficients, from <span class="math notranslate nohighlight">\(\beta_0\)</span> to <span class="math notranslate nohighlight">\(\beta_d\)</span>, can be estimated using methods similar to multiple linear regression. The design matrix contains terms for the intercept and for each power of the predictor up to the chosen degree d.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\left ( \begin{array}{c} y_1 \\ y_2 \\ \cdots \\ y_n \end{array} \right )_{n \times 1} =  \left ( \begin{array}{ccccc}  1 &amp; x_1 &amp;x_1^2  &amp; \cdots &amp;x_1^d \\
1 &amp; x_2 &amp; x_2^2 &amp; \cdots &amp; x_2^d \\
&amp; &amp; &amp; &amp; \\
1 &amp; x_n &amp; x_n^2 &amp; \cdots &amp; x_n^d
\end{array} \right )_{n \times (d+1)} \ \left ( \begin{array}{c} \beta_0 \\ \cdots \\ \beta_d \end{array} \right )_{(d+1)\times 1} + \text{err}.\end{split}\]</div>
<p><strong>Interpretation of Coefficients</strong>: When using polynomial regression, it’s often more important to look at the fitted curve or the predicted values rather than the estimated coefficients. This is because the coefficients depend on the choice of the basis function. Different methods can yield different coefficients but result in the same predicted curve.</p>
</section>
<section id="choosing-the-degree">
<h2><span class="section-number">5.1.2. </span>Choosing the Degree<a class="headerlink" href="#choosing-the-degree" title="Link to this heading"></a></h2>
<p>When performing polynomial regression, one of the most critical decisions is selecting the appropriate degree <span class="math notranslate nohighlight">\(d\)</span> for the polynomial.</p>
<p>Note that the residual sum of squares, being a measure of training error, is unsuitable for this purpose. Employing it could misleadingly favor the highest degree.</p>
<p>Two common strategies employed are the forward and backward approaches:</p>
<ol class="arabic">
<li><p>Forward Approach:</p>
<blockquote>
<div><ul class="simple">
<li><p>Starting Point: Initiate with the simplest model, typically a linear regression.</p></li>
<li><p>Progression: Incrementally introduce higher-degree polynomial terms to the model.</p></li>
<li><p>Stopping Criterion: Continue adding terms until the coefficient of the latest added polynomial term is statistically insignificant.</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Backward Approach:</p>
<blockquote>
<div><ul class="simple">
<li><p>Starting Point: Begin with a model that has a high degree d. This choice is based on a preliminary understanding or hypothesis about the complexity of the underlying relationship.</p></li>
<li><p>Progression: Gradually eliminate the highest-degree terms from the model.</p></li>
<li><p>Stopping Criterion: Remove terms until the coefficient of the highest-degree term in the model becomes significant. At this juncture, retain the remaining terms and finalize the model.</p></li>
</ul>
</div></blockquote>
</li>
</ol>
<p>Beyond the forward and backward approaches, polynomial degree selection can benefit from algorithms grounded in the AIC, BIC, or Cross-validation methods.</p>
<section id="questioning-the-selection-of-d">
<h3>Questioning the Selection of <span class="math notranslate nohighlight">\(d\)</span><a class="headerlink" href="#questioning-the-selection-of-d" title="Link to this heading"></a></h3>
<p>A logical question that arises is why don’t we evaluate across all possible sub-models, similar to our approach in multiple linear regression?  This prompts a related question: once we’ve determined that d is the highest polynomial order, is there a need to further evaluate the inclusion of lower-order terms?</p>
<p>The answer is no. We don’t typically test the significance of lower-order terms. By default, when opting to fit a polynomial of degree d, we inherently include all lower-order terms. But why?</p>
<p>This practice stems from the non-uniqueness of the design matrix in polynomial regression. In standard multiple linear regression, each column of the design matrix X corresponds to a specific predictor, allowing us to select an optimal set of predictors to explain y. In contrast, the design matrix of a polynomial regression is not unique.</p>
<p>Consider a scenario where the genuine underlying curve corresponds to <span class="math notranslate nohighlight">\(x^2\)</span>. If we fit a polynomial model with a default design matrix (comprising an intercept, linear term, and <span class="math notranslate nohighlight">\(x^2\)</span> term), we would end up with a concise model consisting solely of the <span class="math notranslate nohighlight">\(x^2\)</span>
term, potentially excluding even the intercept. However, if we were to choose a different basis for our design matrix, such as <cite>poly(x)</cite>, the design matrix would also consist of three columns (including the intercept). It’s crucial to remember that the last column isn’t merely <span class="math notranslate nohighlight">\(x^2\)</span>, but a blend of <span class="math notranslate nohighlight">\(x^2\)</span>, <span class="math notranslate nohighlight">\(x\)</span>, and the intercept. As a result, to express <span class="math notranslate nohighlight">\(x^2\)</span>, we need a linear combination of the three columns.</p>
<p>Owing to this inherent complexity, in standard polynomial regression, we generally settle on a degree <span class="math notranslate nohighlight">\(d\)</span> without further testing the lower-order terms. However, there might be specific scenarios or applications where one wishes to evaluate a particular model or formula. In such cases, assessing the significance of certain lower-order terms could be warranted. Otherwise, the preferred practice is simply to determine math:<cite>d</cite>.</p>
</section>
</section>
<section id="implementing-in-r-python">
<h2><span class="section-number">5.1.3. </span>Implementing in R/Python<a class="headerlink" href="#implementing-in-r-python" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Rcode: [<a class="reference external" href="https://liangfgithub.github.io/Rcode_W5_PolynomialRegression.html">Rcode_W5_PolynomialRegression</a>]</p></li>
<li><p>Python: [<a class="reference external" href="https://liangfgithub.github.io/Python_W5_PolynomialRegression.html">Python_W5_PolynomialRegression</a>]</p></li>
</ul>
</section>
<section id="global-vs-local">
<h2><span class="section-number">5.1.4. </span>Global vs Local<a class="headerlink" href="#global-vs-local" title="Link to this heading"></a></h2>
<p>As we conclude this section, I’d like to address some limitations of polynomial regression and introduce potential alternatives.</p>
<p>When dealing with fluctuating data patterns, as one might find in a wavy curve, higher-order polynomials become necessary to adapt to the fluctuations. However, such polynomials are often discouraged in practical applications for several reasons:</p>
<ol class="arabic simple">
<li><p><strong>Interpretability</strong>: It’s  challenging to convey to clients or stakeholders that the dependent variable is influenced by a predictor raised to, say, the eighth power.</p></li>
<li><p><strong>Tail Behavior</strong>: With polynomial regression, predictions beyond the range of the training data can be problematic. Imagine the bulk of our data is clustered in a specific range. When attempting to predict values outside this range, the tail behavior of polynomials, which is dominated by the coefficients of the highest order terms, can result in exaggerated increases or decreases. This makes out-of-range predictions particularly unreliable.</p></li>
<li><p><strong>Global Assumptions</strong>: Polynomial regression inherently posits a global assumption about the mean function, which represents the expected value of the response variable Y given X. Such global functions can lack flexibility, particularly when data exhibits localized fluctuations. For instance, data might exhibit local fluctuations in one range while being relatively flat in another. A global polynomial might struggle to capture these distinct behaviors effectively.</p></li>
</ol>
<p>To address these limitations, we will explore extensions termed as “local polynomial methods.”</p>
<ul class="simple">
<li><p>Firstly, we’ll delve into spline models, which replace the global polynomial function with piece-wise local polynomial functions.</p></li>
<li><p>Secondly, we’ll introduce an approach that modifies the fitting procedure itself. Instead of a consistent linear regression model, we’ll fit localized models at each data point, allowing the coefficients to change based on the value of X.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w5_index.html" class="btn btn-neutral float-left" title="5. Nonlinear Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w5_2_reg_spline.html" class="btn btn-neutral float-right" title="5.2. Regression Splines" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>