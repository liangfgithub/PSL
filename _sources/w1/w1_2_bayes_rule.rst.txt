Compute Bayes rule
===============================

The Bayes classification rule is derived from Bayes' Theorem. Below is the probabilistic description of the data generating process for Example 1. The first line defines the distribution of Y, while the following two lines describe the conditional distribution of X given Y. Multiplying them provides the joint distribution of X and Y.


.. math::
	Y 			&\sim \textsf{Bern}(p), \\
	X \mid  Y=0 &\sim \textsf{N}(\mu_0, \sigma^2 \mathbf{I}_2), \\
 	X \mid  Y=1 &\sim  \textsf{N}(\mu_1, \sigma^2 \mathbf{I}_2).


For predictions, we care about Y given X, the conditional probability. Since Y takes only two possible values, knowing the probability of Y = 1 given X is enough for us to form our best prediction for Y. 

Before applying the Bayes' theorem to compute this conditional probability, I want to clarify some terms.

We often discuss two types of random variables in class: 

* **Discrete Random Variables**: Like Y in our example, we describe its distribution using a probability mass function (PMF).
* **Continuous Random Variables**: Like X,  we describe its distribution using a probability density function (PDF). For example, the PDf of a normal distribution is bell-shaped.

The joint distribution of X and Y here presents a challenge because it's neither purely discrete nor continuous. For discrete variables, we can discuss probabilities of specific values (e.g., Y=1). For continuous variables, however, the probability of a specific value is always zero. In the derivation below,  I'll treat X as discrete, utilizing its density function as the corresponding PMF; this can be justified by discretizing a continuous variable.


The conditional probability is calculated as the joint divided by the marginal.

.. math::

	& P(Y=1 \mid X=x) = \frac{P(Y=1, X=x)}{P(X=x)} \\
	= & \frac{P(Y=1, X=x)}{P(Y=1, X=x) + P(Y=0, X=x)} \\
	=&  \frac{P(Y=1) P( X=x | Y=1)}{P(Y=1) P( X=x | Y=1) + P(Y=0) P( X=x | Y=0)} \\
	=& \frac{(p) \left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^2 \exp \big \{ - \frac{\| x - \mu_1\|^2}{2 \sigma^2}\big \}}{(p) \left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^2 \exp \big \{ - \frac{\| x - \mu_1\|^2}{2 \sigma^2}\big \} + (1-p) \left ( \frac{1}{\sqrt{2 \pi \sigma^2}} \right)^2 \exp \big \{ - \frac{\| x - \mu_0\|^2}{2 \sigma^2}\big \}} \\
	=& \left [ 1 +  \exp \Big \{ \frac{1}{2 \sigma^2} ( \| x- \mu_1\|^2 - \| x - \mu_0\|^2) - \log \frac{p}{1-p} \Big \}  \right ]^{-1}


After simplifying, the conditional probability is found to be a specific expression. This helps us establish the the optimal decision rule (i.e., the Bayes rule) for classification, 

.. math::
	
	&  P(Y=1 \mid X=x)  > 0.5 \\
	\Longleftrightarrow & \frac{1}{2 \sigma^2} ( \| x- \mu_1\|^2 - \| x - \mu_0\|^2) < \log \frac{p}{1-p}


If p=0.5,  log p/(1-p)=0, then we basically predict Y=1 if :math:`\| x - \mu_1\|^2 <  \| x - \mu_0\|^2`. That is, we predict x to be class 1 if it's close to the center of class 1 and 0 if it's closer to the center of class 0. 

The decision rule looks like a quadratic function of x, but it can be simplied as a linear function of x (i.e., the optimal decision boundary for Example 1 is linear):

.. math::

	& \| x - \mu_1\|^2 - \| x - \mu_0\|^2 \\
	= &  \| x\|^2 - 2 x^t \mu_1 + \| \mu_1\|^2 - ( \| x\|^2 - 2 x^t \mu_0 + \| \mu_0\|^2) \\
	= & \| \mu_1\|^2 -  \| \mu_0\|^2 - 2 x^t (\mu_1 - \mu_0)


You will be asked to implement the Bayes rule for Example 2, where given Y = 1, follows a mixture of 10 normal distributions. The derivation remains roughly the same, except that you need to replace the PDF of X given by Y by an average of 10 normals. 


