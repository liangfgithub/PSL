<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>4.1. Regression Trees &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4.2. Random Forest" href="w4_2_randomforest.html" />
    <link rel="prev" title="4. Regression Trees and Ensemble" href="w4_index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../w1/w1_index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="w4_index.html">4. Regression Trees and Ensemble</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">4.1. Regression Trees</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction-to-regression-trees">4.1.1. Introduction to Regression Trees</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#terminology">Terminology</a></li>
<li class="toctree-l4"><a class="reference internal" href="#advantages-of-tree-based-models">Advantages of Tree-Based Models</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-build-a-tree">4.1.2. How to Build a Tree</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#assigning-predictions-to-leaf-nodes">Assigning Predictions to Leaf Nodes</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deciding-where-to-split">Deciding Where to Split</a></li>
<li class="toctree-l4"><a class="reference internal" href="#building-trees">Building Trees</a></li>
<li class="toctree-l4"><a class="reference internal" href="#handling-categorical-predictors">Handling Categorical Predictors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#handling-missing-values">Handling Missing Values</a></li>
<li class="toctree-l4"><a class="reference internal" href="#stopping-criteria">Stopping Criteria</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-prune-complexity-cost">4.1.3. How to Prune: Complexity Cost</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-prune-the-weakest-link-algorithm">4.1.4. How to Prune: the Weakest Link Algorithm</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#non-uniqueness-of-optimal-subtree">Non-Uniqueness of Optimal Subtree</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#how-to-prune-cross-validation">4.1.5. How to Prune: Cross-Validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#r-python-code-for-regress-trees">4.1.6. R/Python Code for Regress Trees</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="w4_2_randomforest.html">4.2. Random Forest</a></li>
<li class="toctree-l2"><a class="reference internal" href="w4_3_gbm.html">4.3. GBM</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w5/w5_index.html">5. Nonlinear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w6/w6_index.html">6. Clustering Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w7/w7_index.html">7. Latent Structure Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w9/w9_index.html">9. Discriminant Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w11/w11_index.html">11. Support Vector Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w4_index.html"><span class="section-number">4. </span>Regression Trees and Ensemble</a></li>
      <li class="breadcrumb-item active"><span class="section-number">4.1. </span>Regression Trees</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="regression-trees">
<h1><span class="section-number">4.1. </span>Regression Trees<a class="headerlink" href="#regression-trees" title="Link to this heading"></a></h1>
<section id="introduction-to-regression-trees">
<h2><span class="section-number">4.1.1. </span>Introduction to Regression Trees<a class="headerlink" href="#introduction-to-regression-trees" title="Link to this heading"></a></h2>
<p>This week, we’ll delve into tree-based models for regression. We’ll start by discussing how to construct a single regression tree before progressing to ensemble methods like Random Forests, which rely on bagging, and Gradient Boosting Machines (GBM), based on boosting techniques. Tree models serve a critical role in supervised learning, and we’ll revisit them in future discussions on classification.</p>
<p>To set the stage for regression, consider an input feature vector X with p predictors (excluding the intercept). Additionally, we have a response variable Y that takes on numerical values.</p>
<p>Trees models are constructed by recursively splitting regions of X  into two sub-regions, beginning with the entire feature space X. For simplicity, we’ll focus on recursive binary partitions.</p>
<p>To illustrate the concept of a regression tree, let’s examine a fitted regression tree based on the Boston Housing Data. We’ll focus specifically on two features: longitude and latitude.</p>
<a class="reference internal image-reference" href="../_images/w4_plot_housing_lon_alt.png"><img alt="../_images/w4_plot_housing_lon_alt.png" src="../_images/w4_plot_housing_lon_alt.png" style="width: 564.0px; height: 351.20000000000005px;" /></a>
<ul class="simple">
<li><p>On the right-hand plot, each dot represents a house, positioned according to its longitude and latitude. The greyscale indicates the price, with darker dots representing more expensive houses.</p></li>
<li><p>On the left-hand plot, you see the regression tree displayed in an upside-down manner. The tree starts at the root and recursively splits the feature space into narrower regions, accounting for both longitude and latitude, thus allowing for highly localized predictions.</p></li>
</ul>
<p>Once the tree is built, each leaf node, corresponding to a rectangular region in the feature space, is assigned a constant prediction. For example, one leaf node may predict houses within its region to have a log-scale price of 3.3.</p>
<section id="terminology">
<h3>Terminology<a class="headerlink" href="#terminology" title="Link to this heading"></a></h3>
<p>Before we proceed, let’s clarify some terminology:</p>
<ul class="simple">
<li><p><strong>Leaf Node</strong>: Each leaf node corresponds to a rectangular-shaped region in the feature space.</p></li>
<li><p><strong>Internal Node</strong>: These nodes are parent nodes with two children—left and right nodes.</p></li>
<li><p>A “<strong>split</strong>” involves a variable j and a particular split value s.</p></li>
</ul>
</section>
<section id="advantages-of-tree-based-models">
<h3>Advantages of Tree-Based Models<a class="headerlink" href="#advantages-of-tree-based-models" title="Link to this heading"></a></h3>
<p>One of the major advantages of tree-based models is their interpretability. They can be easily explained and understood, even by a non-technical audience. Additionally, tree models perform automatic variable selection and account for interactions among variables at different levels of the tree.</p>
<p>Furthermore, there’s no need to apply monotonic transformations to predictors, such as square root or log transformations. This is because tree models are invariant to such transformations. For example, suppose we decide to split the variable X_j at a value s, and all values are positive. Applying a square root transformation to X_j would simply involve taking the square root of s, leading to equivalent splits.</p>
</section>
</section>
<section id="how-to-build-a-tree">
<h2><span class="section-number">4.1.2. </span>How to Build a Tree<a class="headerlink" href="#how-to-build-a-tree" title="Link to this heading"></a></h2>
<p>Building a regression tree involves three core elements:</p>
<ul class="simple">
<li><p>Where to Split: Identifying where to divide your feature space.</p></li>
<li><p>When to Stop: Deciding when to stop growing the tree.</p></li>
<li><p>How to Predict at Leaf Nodes: Deciding how to assign predictions to each leaf node.</p></li>
</ul>
<section id="assigning-predictions-to-leaf-nodes">
<h3>Assigning Predictions to Leaf Nodes<a class="headerlink" href="#assigning-predictions-to-leaf-nodes" title="Link to this heading"></a></h3>
<p>This last question might be the easiest to answer.  When you focus on minimizing the squared error between observed and predicted values, the best prediction for each leaf node is simply the average response of the samples in that node.</p>
</section>
<section id="deciding-where-to-split">
<h3>Deciding Where to Split<a class="headerlink" href="#deciding-where-to-split" title="Link to this heading"></a></h3>
<p>The core decision in growing a tree is determining where to split. Each potential split is represented by a variable j and value s. A split score, or criterion, is defined to evaluate the quality of the split. We consider each predictor variable and all possible split values, selecting the one that optimizes the split score.</p>
<p>For regression trees, a popular split score is based on the “gain in residual sum of squares,” i.e., how much the residual sum of squares (RSS) decreases because of the split.</p>
<ul class="simple">
<li><p><strong>Without a Split</strong>: Consider a node t with 20 samples. If we choose not to split, we predict the response for each observation in the node using the average Y value of the 20 observations, and then calculate the RSS from these predictions.</p></li>
<li><p><strong>With a split</strong>: If we split using variable j and value s, the 20 observations are divided into two groups: left and right. Let’s assume the left node contains 15 observations and the right node contains 5. For the left node, we predict the Y values of the 15 observations based on their average, and then calculate the RSS for this node. The RSS can be computed similarly for the right node, where we predict the Y values of the remaining 5 observations based on their average. The total new RSS is the sum of the RSS for the left and right nodes. Importantly, this new RSS is always smaller than the original RSS calculated without a split, as the split always provides a better fit to the data.</p></li>
</ul>
</section>
<section id="building-trees">
<h3>Building Trees<a class="headerlink" href="#building-trees" title="Link to this heading"></a></h3>
<p>Trees are built in a top-down and greedy fashion. For each node, we try all possible splits (both features and split values) and pick the one that provides the most significant gain in RSS.</p>
</section>
<section id="handling-categorical-predictors">
<h3>Handling Categorical Predictors<a class="headerlink" href="#handling-categorical-predictors" title="Link to this heading"></a></h3>
<p>When dealing with a categorical variable that has m levels, one might initially think that there would be numerous ways to divide these m levels into two groups, which would make the problem computationally expensive. However, when constructing a regression tree that minimizes RSS, the situation is much simpler than it seems.</p>
<p>The simplified answer is that we sort the m levels by their corresponding means of the response variable Y. After sorting, we only need to consider m-1 possible splits or “cuts,” which are made between adjacent levels.</p>
<p>Why does this work? The intuition is that, when it comes to minimizing RSS (in the case of regression trees), we would ideally like to group together levels that have similar means, as this minimizes within-group variance. The sorting process lines up the levels in such a way that adjacent levels are the most similar in terms of the response variable Y, which simplifies the number of splits we need to consider to m-1.</p>
<p><strong>Note</strong>: It is crucial to recognize that these ‘m−1’ are chosen as the most effective among <span class="math notranslate nohighlight">\(2^m-1\)</span> possibilities. Although this selection process itself can be performed quite efficiently, that shouldn’t overshadow the fact that a categorical variable with a large number of levels (‘m’) has significantly greater partitioning power compared to a numerical feature. This increased partitioning capability can make the model more susceptible to fitting the noise in the data, thereby elevating the risk of overfitting.</p>
</section>
<section id="handling-missing-values">
<h3>Handling Missing Values<a class="headerlink" href="#handling-missing-values" title="Link to this heading"></a></h3>
<p>Unlike some other algorithms that require complete data, tree-based methods like regression trees offer various strategies to deal with missing data, such as</p>
<ul class="simple">
<li><p>Missing Value as a Separate Category: One simple approach is to treat missing values as a separate category for categorical variables or as a separate range for numerical variables. This method doesn’t require any imputation.</p></li>
<li><p>Surrogate Splits: This is a more sophisticated approach. When a primary split is made on a predictor variable, one or more surrogate variables can be selected as backups. If the primary variable has a missing value for a particular observation, the surrogate variable is used to make the split for that observation. This is particularly effective when the surrogate variables are strongly correlated with the primary variable.</p></li>
<li><p>Blind/Majority Rule: Some implementations simply assign missing values to the most populous child node after the split. The rationale is to minimize the influence of missing values by assigning them to a “safe” or majority group.</p></li>
<li><p>Imputation: The missing values can be imputed before building the tree. Various imputation methods can be used, such as mean imputation, median imputation, or using another predictive model to estimate the missing value. However, imputed values are not always reliable and can introduce bias.</p></li>
</ul>
</section>
<section id="stopping-criteria">
<h3>Stopping Criteria<a class="headerlink" href="#stopping-criteria" title="Link to this heading"></a></h3>
<p>We don’t always want to keep splitting. A simplistic approach is to stop when the gain in RSS falls below a pre-defined threshold. Another method is to first build a large tree and then prune it back.</p>
</section>
</section>
<section id="how-to-prune-complexity-cost">
<h2><span class="section-number">4.1.3. </span>How to Prune: Complexity Cost<a class="headerlink" href="#how-to-prune-complexity-cost" title="Link to this heading"></a></h2>
<p>The essential idea behind pruning is straightforward: start with a “big” or “complex” initial tree that has captured various facets of the data, and then trim it down. We do this in a systematic manner that optimizes a particular measure known as the ‘complexity cost.’</p>
<p>The complexity cost for a tree T is defined as:</p>
<div class="math notranslate nohighlight">
\[R_{\alpha}(T) = \text{RSS}(T) + \alpha |T|\]</div>
<p>where</p>
<ul class="simple">
<li><p>RSS(T) is the RSS for the tree. This measures how well the tree fits the data.</p></li>
<li><p><span class="math notranslate nohighlight">\(|T|\)</span> is the number of leaf nodes in the tree. This quantifies the size or complexity of the tree.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is a non-negative tuning parameter that controls the trade-off between the fit and the complexity. A larger <span class="math notranslate nohighlight">\(\alpha\)</span>  will result in a simpler tree, while a smaller <span class="math notranslate nohighlight">\(\alpha\)</span> will fit the data more closely but at the risk of overfitting.</p></li>
</ul>
<p>For a  given value of <span class="math notranslate nohighlight">\(\alpha\)</span>, the sub-tree that minimizes the complexity cost is selected as the optimal tree. However, it’s important to note that the minimizer may not be unique.  If multiple sub-trees T_1, T_2, …, T_m yield the same minimal complexity cost,  the Weakest Link Algorithm will reveal that there exists a “smallest” tree, which is essentially the intersection of these m sub-trees. By defining the optimal sub-tree as this smallest intersecting tree, we ensure its uniqueness.</p>
</section>
<section id="how-to-prune-the-weakest-link-algorithm">
<h2><span class="section-number">4.1.4. </span>How to Prune: the Weakest Link Algorithm<a class="headerlink" href="#how-to-prune-the-weakest-link-algorithm" title="Link to this heading"></a></h2>
<p>To find the optimal sub-tree that minimizes the complexity cost, it’s essential to understand the relationship between the cost and the value of <span class="math notranslate nohighlight">\(\alpha\)</span>, which serves as the penalty for adding a split to the tree.</p>
<p>Consider a split originating from an internal node t, leading to leaf nodes t_R and t_L. There exists a threshold <span class="math notranslate nohighlight">\(\alpha^*\)</span> that signifies the maximum price we’re willing to pay to maintain this split. If the cost exceeds this threshold, the branch should be collapsed or trimmed. Conversely, if the cost is acceptable, the split should be retained.</p>
<p><span class="math notranslate nohighlight">\(\alpha^*\)</span> can be calculated through the improvement in the RSS. This is the difference between the RSS when all observations are clustered at node t, and when the data is distributed into two nodes. <span class="math notranslate nohighlight">\(\alpha^*\)</span> is precisely equivalent to this improvement in RSS. When the cost is low enough, the improvement offsets the price, and the split is kept. If the cost is too high, the split is removed.</p>
<p>Now let’s extend this concept to calculate the maximum price we are willing to pay to keep each branch intact.</p>
<a class="reference internal image-reference" href="../_images/w4_alpha_calculation.png"><img alt="../_images/w4_alpha_calculation.png" src="../_images/w4_alpha_calculation.png" style="width: 468.0px; height: 326.0px;" /></a>
<p>The <strong>Weakest Link Algorithm</strong> can be used to find the optimal subtree for all possible values of <span class="math notranslate nohighlight">\(\alpha\)</span>. The algorithm proceeds as follows:</p>
<ol class="arabic simple">
<li><p>Start with a large tree and initialize <span class="math notranslate nohighlight">\(\alpha\)</span> to zero.</p></li>
<li><p>Calculate the maximum <span class="math notranslate nohighlight">\(\alpha\)</span> value for each non-leaf internal node, denoting the maximum price we’re willing to pay to keep the branches downstream of that node.</p></li>
<li><p>Identify the smallest of these <span class="math notranslate nohighlight">\(\alpha\)</span> values, <span class="math notranslate nohighlight">\(\alpha_1\)</span>, and trim the corresponding branch if <span class="math notranslate nohighlight">\(\alpha\)</span> exceeds <span class="math notranslate nohighlight">\(\alpha_1\)</span>.</p></li>
<li><p>Update the maximum <span class="math notranslate nohighlight">\(\alpha\)</span> calculations for the remaining internal nodes and repeat the process.</p></li>
</ol>
<p>Step 4 is necessary because when a branch is pruned, it impacts the maximal <span class="math notranslate nohighlight">\(\alpha\)</span> values for internal nodes that are upstream of the cut branch.</p>
<p>The algorithm generates a sequence of increasingly smaller trees corresponding to increasing <span class="math notranslate nohighlight">\(\alpha\)</span> values, starting from the largest tree down to a single-node tree (the root).</p>
<a class="reference internal image-reference" href="../_images/w4_solution_path.png"><img alt="../_images/w4_solution_path.png" src="../_images/w4_solution_path.png" style="width: 440.5px; height: 345.5px;" /></a>
<section id="non-uniqueness-of-optimal-subtree">
<h3>Non-Uniqueness of Optimal Subtree<a class="headerlink" href="#non-uniqueness-of-optimal-subtree" title="Link to this heading"></a></h3>
<p>The Weakest Link Algorithm also reveals that for any given value of <span class="math notranslate nohighlight">\(\alpha\)</span>, if the maximal price for retaining a branch is precisely equal to <span class="math notranslate nohighlight">\(\alpha\)</span>, the resulting optimal subtree will not be unique. In such cases, one could either keep the branch or trim it away. This scenario could occur for multiple branches, leading to multiple optimal subtrees. However, this also implies that there exists a smallest optimal subtree, which is essentially the tree formed by trimming these ambiguous branches. This smallest optimal subtree, which is the intersection of all such optimal solutions, is unique.</p>
</section>
</section>
<section id="how-to-prune-cross-validation">
<h2><span class="section-number">4.1.5. </span>How to Prune: Cross-Validation<a class="headerlink" href="#how-to-prune-cross-validation" title="Link to this heading"></a></h2>
<p>We have generated a solution path that encompasses all possible values of <span class="math notranslate nohighlight">\(\alpha\)</span>. The next challenge is to determine which <span class="math notranslate nohighlight">\(\alpha\)</span> value to use. To make this decision, we’ll employ a strategy similar to those used in Ridge and Lasso regressions: Cross-Validation.</p>
<p>Here’s an overview of the cross-validation procedure as implemented in the R package ‘rpart,’ which we’ll be using to fit regression trees:</p>
<ol class="arabic">
<li><p>Initial Tree Fitting: First, we fit a comprehensive tree to the data and identify various intervals, each corresponding to a set of <span class="math notranslate nohighlight">\(\alpha\)</span> values.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span> Value Calculation: Instead of using the exact <span class="math notranslate nohighlight">\(\alpha\)</span> values that border each interval, we compute a <span class="math notranslate nohighlight">\(\beta\)</span> value within each interval. For example, <span class="math notranslate nohighlight">\(\beta_1\)</span> is calculated as the square root of <span class="math notranslate nohighlight">\(\alpha_1 \times \beta_2\)</span>, placing it within the given interval.</p></li>
<li><p>Cross-Validation: With these <span class="math notranslate nohighlight">\(\beta\)</span> values identified, we initiate the K-fold cross-validation. We partition the dataset into K groups and carry out the following steps for each subset k:</p>
<blockquote>
<div><ol class="loweralpha simple">
<li><p>Model Fitting: We fit a regression tree using K−1 folds, leaving out the k-th group of data.</p></li>
<li><p>Optimal Tree Identification: Utilizing the pre-determined <span class="math notranslate nohighlight">\(\beta\)</span> values, we find the optimal tree structure.</p></li>
<li><p>Prediction Error: We calculate the prediction error for each tree on the observations in the left-out k-th group.</p></li>
</ol>
</div></blockquote>
</li>
</ol>
<p>It’s crucial to note that while the same <span class="math notranslate nohighlight">\(\beta\)</span> values are used to trim the trees for all K groups, the resultant tree sizes may differ for each k.</p>
<p>After running the procedure for all K groups, we can calculate the Cross-Validation (CV) error as a function of <span class="math notranslate nohighlight">\(\alpha\)</span>. Finally, we select the optimal <span class="math notranslate nohighlight">\(\alpha\)</span> value based on either the minimum CV error or using the one standard error principle.</p>
</section>
<section id="r-python-code-for-regress-trees">
<h2><span class="section-number">4.1.6. </span>R/Python Code for Regress Trees<a class="headerlink" href="#r-python-code-for-regress-trees" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Rcode: [<a class="reference external" href="https://liangfgithub.github.io/Rcode_W4_RegressionTree.html">Rcode_W4_RegressionTree</a>]</p></li>
<li><p>Python: [<a class="reference external" href="https://liangfgithub.github.io/Python_W4_RegressionTree.html">Python_W4_RegressionTree</a>]</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w4_index.html" class="btn btn-neutral float-left" title="4. Regression Trees and Ensemble" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w4_2_randomforest.html" class="btn btn-neutral float-right" title="4.2. Random Forest" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>