<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>9.4. Linear Discriminant Analysis &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.5. Fisher Discriminant Analysis" href="w9_5_FDA.html" />
    <link rel="prev" title="9.3. Quadratic Discriminant Analysis" href="w9_3_QDA.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../w1/w1_index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w4/w4_index.html">4. Regression Trees and Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w5/w5_index.html">5. Nonlinear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w6/w6_index.html">6. Clustering Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w7/w7_index.html">7. Latent Structure Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="w9_index.html">9. Discriminant Analysis</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="w9_1_intro_classification.html">9.1. Introduction to Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_2_DA.html">9.2. Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_3_QDA.html">9.3. Quadratic Discriminant Analysis</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">9.4. Linear Discriminant Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#reduced-rank-lda">9.4.1. Reduced Rank LDA</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="w9_5_FDA.html">9.5. Fisher Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_6_NB.html">9.6. Naive Bayes Classifiers</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_7_summ.html">9.7. Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w11/w11_index.html">11. Support Vector Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w9_index.html"><span class="section-number">9. </span>Discriminant Analysis</a></li>
      <li class="breadcrumb-item active"><span class="section-number">9.4. </span>Linear Discriminant Analysis</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="linear-discriminant-analysis">
<h1><span class="section-number">9.4. </span>Linear Discriminant Analysis<a class="headerlink" href="#linear-discriminant-analysis" title="Link to this heading"></a></h1>
<p>In our previous discussion on Quadratic Discriminant Analysis (QDA), the discriminant function plays a pivotal role in making classification decisions. If we make the assumption that all groups share the same covariance matrix (<span class="math notranslate nohighlight">\(\Sigma_k = \Sigma\)</span>), the discriminant function can be expressed as follows:</p>
<div class="math notranslate nohighlight">
\[d_k (x) =  (x-\mu_k)^t \Sigma^{-1} (x-\mu_k)  + \textcolor{gray}{\log |\Sigma|} -2  \log \pi_k\]</div>
<p>The first term remains a quadratic function, measuring the Mahalanobis distance between the data point <span class="math notranslate nohighlight">\(x\)</span> and the class center <span class="math notranslate nohighlight">\(\mu_k\)</span>, followed by the determinant of <span class="math notranslate nohighlight">\(\Sigma\)</span> and minus two times the logarithm of <span class="math notranslate nohighlight">\(\pi_k\)</span>. Note that the logarithm of <span class="math notranslate nohighlight">\(\Sigma\)</span> is grayed out because it is a common term for all classes and doesn’t affect the final decision.</p>
<p>Further simplification reveals that the first term, discounting the gray component, is linear in <span class="math notranslate nohighlight">\(x\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp; (x-\mu_k)^t \Sigma^{-1} (x-\mu_k) \\
=&amp;  \textcolor{gray}{x^t \Sigma^{-1} x} - 2 x^t \Sigma^{-1} \mu_k +  \mu_k^T \Sigma^{-1} \mu_k\end{split}\]</div>
<p>When we assume that all covariance matrices are the same for all classes, we essentially revert to Linear Discriminant Analysis (LDA) since the discriminant function becomes linear. Consequently, the decision boundary for binary classification is determined by a linear function of <span class="math notranslate nohighlight">\(x\)</span>, leading to the name LDA.</p>
<p>In practice, the estimation of <span class="math notranslate nohighlight">\((\pi_k, \mu_k)\)</span> can be carried out similarly to how it’s done in QDA. As for estimating the shared covariance matrix <span class="math notranslate nohighlight">\(\Sigma\)</span>, a common approach involves pooled sample covariance estimation. Here, ‘pooled’ refers to the fact that information about covariance is gathered from various classes.</p>
<div class="math notranslate nohighlight">
\[\hat{\Sigma}= \frac{1}{n-K} \sum_{k=1}^K \sum_{i: y_i=k} (x_i -  \hat{\mu}_k) (x_i -
\hat{\mu}_k)^t\]</div>
<p>In the discriminant function, what matters is <span class="math notranslate nohighlight">\(\hat{\Sigma}^{-1}\)</span>. A concern arises if <span class="math notranslate nohighlight">\(\hat{\Sigma}\)</span> is not invertible. The rank of <span class="math notranslate nohighlight">\(\hat{\Sigma}\)</span> is at most n-K. Thus, this situation can arise when the dimensionality ‘p’ is substantial.</p>
<p>To address non-invertibility, we can employ the same trick used in QDA. By adding a small constant <span class="math notranslate nohighlight">\(\epsilon\)</span> times the identity matrix to <span class="math notranslate nohighlight">\(\hat{Sigma}\)</span>, we ensure its invertibility.</p>
<p>Alternatively, one can compute the generalized inverse of the matrix. The generalized inverse essentially ignores dimensions where the matrix is singular, allowing us to calculate the inverse only for the dimensions where it is possible.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{\Sigma} = U_{p \times 3} \left ( \begin{array}{ccc} d_1 &amp; 0 &amp; 0 \\ 0 &amp; d_2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array} \right ) U^t, \quad \hat{\Sigma}^{-1} = U  \left ( \begin{array}{ccc} 1/d_1 &amp; 0 &amp; 0 \\ 0 &amp; 1/d_2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{array}  \right ) U^t\end{split}\]</div>
<p>This approach works well for LDA because it involves a shared covariance matrix across all classes. However, it may not apply to QDA, where different covariance matrices might have different null spaces.</p>
<section id="reduced-rank-lda">
<h2><span class="section-number">9.4.1. </span>Reduced Rank LDA<a class="headerlink" href="#reduced-rank-lda" title="Link to this heading"></a></h2>
<p>Linear Discriminant Analysis (LDA) offers a way to naturally reduce the dimensionality of a classification problem. L</p>
<p>Let’s start by assuming that <span class="math notranslate nohighlight">\(\Sigma\)</span> is an identity matrix, which simplifies the LDA discriminant function. In this simplified form, the first term in the discriminant function is the Euclidean distance from the point <span class="math notranslate nohighlight">\(x\)</span> to the class center, <span class="math notranslate nohighlight">\(\mu_k\)</span>, due to the identity covariance matrix.</p>
<div class="math notranslate nohighlight">
\[d_k (x) =  \| x-\mu_k \|^2  -2  \log \pi_k.\]</div>
<p>Without loss of generality, let’s assume the average of the K centers is the origin. This choice stems from the fact that what truly matters is the relative distances between data points, and we have the flexibility to designate the origin’s location arbitrarily. Consequently, the K class centers, <span class="math notranslate nohighlight">\((\mu_1, \dots, \mu_K)\)</span>, form a (K-1)-dimensional subspace. This leads us to substitute the first term, <span class="math notranslate nohighlight">\(\| x-\mu_k \|^2\)</span>, in the original p-dimensional space, with a squared distance calculated  within the lower-dimensional (K-1)-dimensional subspace. This transformation naturally induces dimension reduction, effectively reducing the initial dimensionality denoted as p to (K-1).</p>
<p>But why is this feasible? We can denote the subspace defined by the K centers as <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>. For any p-dimensional vector <span class="math notranslate nohighlight">\(x\)</span>, we can decompose it as a sum of two orthogonal vectors:</p>
<div class="math notranslate nohighlight">
\[x = x_1 + x_2.\]</div>
<p>Here, <span class="math notranslate nohighlight">\(x_1\)</span> belongs to the (K-1)-dimensional subspace, <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>, while <span class="math notranslate nohighlight">\(x_2\)</span> resides in the  orthogonal complement, which has a dimension of (p-K+1). Consequently, the first term in  <span class="math notranslate nohighlight">\(d_k(x)\)</span> can be expressed as:</p>
<div class="math notranslate nohighlight">
\[\| x-\mu_k \|^2 = \| x_1 + x_2 - \mu_k\|^2 = \| x_1  - \mu_k\|^2 + \|x_2\|^2.\]</div>
<p>Notably, for any <span class="math notranslate nohighlight">\(x\)</span>, the term <span class="math notranslate nohighlight">\(\|x_2\|^2\)</span> remains constant. Given that our primary concern is determining which ‘k’ yields the largest <span class="math notranslate nohighlight">\(d_k\)</span> for any <span class="math notranslate nohighlight">\(x\)</span>, we can safely disregard <span class="math notranslate nohighlight">\(\|x_2\|^2\)</span>.  As a result, we can perform LDA exclusively within this reduced (K - 1)-dimensional space.</p>
<p>For instance, consider a special case where K equals two, representing a binary classification problem with an original dimensionality p of two. Even though the data exists in a two-dimensional space, we can perform classification by projecting all data points onto the x-coordinate and determining their proximity to the respective centers to make decisions. This example illustrates how LDA naturally reduces the dimensionality from the original ‘p’ to K minus one, which, in this extreme case, results in just one dimension.</p>
<a class="reference internal image-reference" href="../_images/w9_LDA_reduced_1.png"><img alt="../_images/w9_LDA_reduced_1.png" src="../_images/w9_LDA_reduced_1.png" style="width: 265.5px; height: 171.0px;" /></a>
<p>It’s important to note that our derivations thus far assume an identity covariance matrix. However, the conclusions remain consistent even when the covariance matrix deviates from identity. A normalization step is needed to transform the data from the ellipsoid shape to a spherical shape. This transformation involves scaling by the square root of the covariance matrix.</p>
<a class="reference internal image-reference" href="../_images/w9_LDA_normalize.png"><img alt="../_images/w9_LDA_normalize.png" src="../_images/w9_LDA_normalize.png" style="width: 516.0px; height: 246.0px;" /></a>
<p>Consider a scenario with a tilted, ellipsoid-shaped contour plot of data clouds. In this case, LDA still leads to the same outcome. Whether in the original two-dimensional space or projected onto a one-dimensional black line, the results will be consistent.</p>
<a class="reference internal image-reference" href="../_images/w9_LDA_reduced_2.png"><img alt="../_images/w9_LDA_reduced_2.png" src="../_images/w9_LDA_reduced_2.png" style="width: 237.75px; height: 188.25px;" /></a>
<p>However, it’s essential to recognize that each of the ‘K minus one’ dimensions learned from LDA is, in fact, a linear combination of all the original p dimensions. This implies that overfitting can occur when p is significantly larger than K. In such cases, while the LDA projection may seem to provide excellent separation in the low-dimensional space during training, it may not generalize well to test data, as demonstrated in our code page.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w9_3_QDA.html" class="btn btn-neutral float-left" title="9.3. Quadratic Discriminant Analysis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w9_5_FDA.html" class="btn btn-neutral float-right" title="9.5. Fisher Discriminant Analysis" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>