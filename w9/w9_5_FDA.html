<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>9.5. Fisher Discriminant Analysis &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=888ff710"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.6. Naive Bayes Classifiers" href="w9_6_NB.html" />
    <link rel="prev" title="9.4. Linear Discriminant Analysis" href="w9_4_LDA.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../w1/w1_index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w4/w4_index.html">4. Regression Trees and Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w5/w5_index.html">5. Nonlinear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w6/w6_index.html">6. Clustering Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w7/w7_index.html">7. Latent Structure Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="w9_index.html">9. Discriminant Analysis</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="w9_1_intro_classification.html">9.1. Introduction to Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_2_DA.html">9.2. Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_3_QDA.html">9.3. Quadratic Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_4_LDA.html">9.4. Linear Discriminant Analysis</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">9.5. Fisher Discriminant Analysis</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#lda-vs-fda">9.5.1. LDA vs. FDA</a></li>
<li class="toctree-l3"><a class="reference internal" href="#some-remedies">9.5.2. Some Remedies</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="w9_6_NB.html">9.6. Naive Bayes Classifiers</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_7_summ.html">9.7. Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w11/w11_index.html">11. Support Vector Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w9_index.html"><span class="section-number">9. </span>Discriminant Analysis</a></li>
      <li class="breadcrumb-item active"><span class="section-number">9.5. </span>Fisher Discriminant Analysis</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="fisher-discriminant-analysis">
<h1><span class="section-number">9.5. </span>Fisher Discriminant Analysis<a class="headerlink" href="#fisher-discriminant-analysis" title="Link to this heading"></a></h1>
<p>In the context of dimension reduction methods, it’s important to distinguish between unsupervised and supervised techniques.</p>
<p>Unsupervised methods like Principal Component Analysis (PCA) operate solely on the dataset X without any consideration for the labels Y, which may not even exist.</p>
<p>On the other hand, in supervised dimension reduction, we do have access to Y, the class labels. The goal is to find a projection direction such that when we project the data onto it, we obtain one-dimensional values that exhibit clear separation among the classes.</p>
<a class="reference internal image-reference" href="../_images/w9_FDA_toydata.png"><img alt="../_images/w9_FDA_toydata.png" src="../_images/w9_FDA_toydata.png" style="width: 279.0px; height: 225.0px;" /></a>
<p>For instance, consider a scenario where data points are in a two-dimensional space, and they belong to three different classes. We aim to identify the best projection direction that maximizes class separation and the worst one that doesn’t. To achieve this, we first need to define what we mean by “being well-separated.”</p>
<p>Being well-separated implies that the means of the data points from different classes are far apart, while the data points within the same class cluster closely around their respective means. This concept can be expressed mathematically as the maximization of a ratio: the ratio of between-group variation to within-group variation after projection. To compute this ratio, let’s assume the direction we’re seeking is represented as vector <span class="math notranslate nohighlight">\(\mathbf{a} \in \mathbb{R}^p\)</span>. After projecting the data onto this direction <span class="math notranslate nohighlight">\(\mathbf{a}\)</span>, the ratio can be calculated as a quadratic expression involving matrices B and W.</p>
<div class="math notranslate nohighlight">
\[\frac{\text{Between group variation}}{\text{Within group variation}} =  \frac{\mathbf{a}^t B \mathbf{a}}{\mathbf{a}^t W \mathbf{a}}.\]</div>
<p>W represents within-class variation, similar to a pooled sample covariance matrix. It accumulates the variations of each data point from its respective class center and sums them over all K classes. B, on the other hand, signifies between-class covariance, measuring the separation of class centers from each other.</p>
<div class="math notranslate nohighlight">
\[\begin{split}W_{\textcolor{gray}{p \times p}} &amp;= \frac{1}{n-K} \sum_{k=1}^K \sum_{i: y_i=k} (x_{i} - \mu_k) (x_{i} - \mu_k)^t \\
B_{\textcolor{gray}{p \times p}} &amp; =   \frac{1}{K-1} \sum_{k=1}^K n_k (\mu_k - \bar{\mu})  (\mu_k - \bar{\mu})^t\end{split}\]</div>
<p>To help understand B and W, consider that every <span class="math notranslate nohighlight">\(x_i\)</span> can be represented as the sum of its own class mean (<span class="math notranslate nohighlight">\(\mu_k\)</span>) and the deviation from its class mean. W captures the variation within the purple points (deviations from class means), while B accounts for the covariance among the blue points (class means).</p>
<div class="math notranslate nohighlight">
\[\begin{split}x_1 &amp;= \textcolor{magenta}{x_1 - \mu_{y_1}} + \textcolor{blue}{ \mu_{y_1}} \\
x_2 &amp;= \textcolor{magenta}{x_2 - \mu_{y_2}} + \textcolor{blue}{\mu_{y_2}} \\
\dots &amp; =  \dots \\
x_n &amp;= \textcolor{magenta}{x_n - \mu_{y_n}} + \textcolor{blue}{\mu_{y_n}}\end{split}\]</div>
<p>Maximize the ratio can be turned into a generalized eigenvalue problem. This problem involves maximizing a quadratic function of B subject to a quadratic constraint on <span class="math notranslate nohighlight">\(\mathbf{a}\)</span>.</p>
<a class="reference internal image-reference" href="../_images/w9_FDA_genEigen_1.png"><img alt="../_images/w9_FDA_genEigen_1.png" src="../_images/w9_FDA_genEigen_1.png" style="width: 511.20000000000005px; height: 219.20000000000002px;" /></a>
<p>The solution <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> yields the first eigenvector of the matrix. By repeating this process iteratively, you can extract K-1 directions, where K is the number of classes. It’s important to note that the rank of B is at most K-1, which limits the number of eigenvectors obtainable.</p>
<a class="reference internal image-reference" href="../_images/w9_FDA_genEigen_2.png"><img alt="../_images/w9_FDA_genEigen_2.png" src="../_images/w9_FDA_genEigen_2.png" style="width: 501.2px; height: 350.0px;" /></a>
<p>A key distinction between <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> vectors and <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> vectors lies in their orthogonality. B vectors are eigenvectors and are orthogonal and normalized, with a norm of one. In contrast, a vectors are derived from b vectors through a transformation and are no longer orthogonal to each other. Additionally, their norms are not necessarily one; they can be considered as being orthogonal in a new space defined by the W matrix, where inner products between the vectors are calculated accordingly.</p>
<p>In summary, Fisher’s Discriminant Analysis (FDA) involves formulating and solving a generalized eigenvalue decomposition problem to find K-1 directions that lead to well-separated data when projected. Notably, these K-1 directions span the same subspace as the one obtained in reduced-rank Linear Discriminant Analysis (LDA), providing a valuable connection between the two techniques. This alignment becomes particularly apparent when within-class covariance is identity. In such cases, the eigenvectors of B correspond to the space where class centers lie after removing the mean, establishing the equivalence between FDA and LDA in this scenario.</p>
<section id="lda-vs-fda">
<h2><span class="section-number">9.5.1. </span>LDA vs. FDA<a class="headerlink" href="#lda-vs-fda" title="Link to this heading"></a></h2>
<p>Let’s take a moment to compare Linear Discriminant Analysis (LDA) and Fisher’s Discriminant Analysis (FDA).</p>
<ul class="simple">
<li><p>LDA is a classification method based on the assumption of a common covariance matrix across different classes.</p></li>
<li><p>FDA is a supervised dimension reduction technique, and its output consists of directions rather than a classification rule.</p></li>
</ul>
<p>An intriguing aspect of these methods is that LDA imposes strict assumptions about the normality of features, while FDA formulates the problem more generally without making specific distributional assumptions. But it turns out that normality assumptions are implicitly embedded in FDA. This implicit inclusion of normality stems from FDA’s reliance on second moments of covariance matrices and centers of the data, which is a characteristic of normal distributions. The parameters of normal distributions involve only means and covariances. In FDA, the within-class covariance matrix assumes that the covariance structure is the same across different classes, which matches the assumption in LDA.</p>
<p>Despite their differences, FDA and LDA share a similarity: the subspaces they produce are equivalent. In fact, in software like R, there isn’t a specific FDA command; you can extract FDA directions from LDA outputs.</p>
<p>Now, let’s delve further into dimension reduction algorithms.</p>
<p>FDA qualifies as a supervised dimension reduction algorithm since it leverages label information, denoted as Y. This differs from Principal Component Analysis (PCA), which is an unsupervised dimension reduction method. In PCA, if you change the labels or add irrelevant high-pigment labels to Y, the PCA directions remain unaffected.</p>
<p>It’s worth noting that FDA, originally designed for classification problems, can also be applied to regression. In regression, the response variable Y is continuous, unlike the discrete labels used in classification. However, you can discretize Y, for instance, by dividing it into quantiles. These quantile-based labels can then be employed for supervised dimension reduction using FDA.</p>
<p>However, when working with FDA, particularly in classification tasks, there’s a risk of overfitting. Let’s illustrate this concern with a simple example:</p>
<p>Imagine six observations, where the first three belong to class one (in blue) and the remaining three to class zero (in red). The features are six-dimensional, although they could be higher-dimensional. The dataset is structured for linear regression with a binary response (1 or 0).</p>
<a class="reference internal image-reference" href="../_images/w9_LDA_overfit.png"><img alt="../_images/w9_LDA_overfit.png" src="../_images/w9_LDA_overfit.png" style="width: 309.5px; height: 268.0px;" /></a>
<p>The goal is to find a regression coefficient vector <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> to predict the response using the design matrix. Given the nature of regression, you can always find such a vector. When you project the data onto this one-dimensional space determined by <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>, you obtain one-dimensional values for each observation.</p>
<p>In this specific example, the projection results in a clear separation between the two classes. Class one points project to one, while class zero points project to zero. However, this separation arises not due to meaningful features but because the feature dimension matches the sample size (n). When the feature dimension equals the sample size, you can generate an n-by-n design matrix and find a direction in the n-dimensional space that effectively separates the classes, even if the features are random noise.</p>
<p>In conclusion, it’s essential to be aware of the potential for overfitting in FDA and similar methods, where seemingly impressive separation may be driven by high-dimensional features rather than meaningful patterns in the data.</p>
</section>
<section id="some-remedies">
<h2><span class="section-number">9.5.2. </span>Some Remedies<a class="headerlink" href="#some-remedies" title="Link to this heading"></a></h2>
<p>To address this problem, there are two common remedies:</p>
<ul class="simple">
<li><dl class="simple">
<dt>L1 Penalty:</dt><dd><p>Add L1 regularization to the optimization problem when estimating covariance matrices or their inverses and mean vectors. This encourages sparsity in the estimated parameters.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Assume Independence:</dt><dd><p>Assume that the p-dimensional density function is composed of independent components. This implies that the covariance matrix (sigma) becomes diagonal. This approach is known as Naive Bayes and can help address high-dimensional overfitting.</p>
</dd>
</dl>
</li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w9_4_LDA.html" class="btn btn-neutral float-left" title="9.4. Linear Discriminant Analysis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w9_6_NB.html" class="btn btn-neutral float-right" title="9.6. Naive Bayes Classifiers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>