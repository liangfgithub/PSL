<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>9.6. Naive Bayes Classifiers &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.7. Summary" href="w9_7_summ.html" />
    <link rel="prev" title="9.5. Fisher Discriminant Analysis" href="w9_5_FDA.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../w1/w1_index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w4/w4_index.html">4. Regression Trees and Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w5/w5_index.html">5. Nonlinear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w6/w6_index.html">6. Clustering Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w7/w7_index.html">7. Latent Structure Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="w9_index.html">9. Discriminant Analysis</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="w9_1_intro_classification.html">9.1. Introduction to Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_2_DA.html">9.2. Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_3_QDA.html">9.3. Quadratic Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_4_LDA.html">9.4. Linear Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_5_FDA.html">9.5. Fisher Discriminant Analysis</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">9.6. Naive Bayes Classifiers</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_7_summ.html">9.7. Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w11/w11_index.html">11. Support Vector Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w9_index.html"><span class="section-number">9. </span>Discriminant Analysis</a></li>
      <li class="breadcrumb-item active"><span class="section-number">9.6. </span>Naive Bayes Classifiers</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="naive-bayes-classifiers">
<h1><span class="section-number">9.6. </span>Naive Bayes Classifiers<a class="headerlink" href="#naive-bayes-classifiers" title="Link to this heading"></a></h1>
<p>What is Naive Bayes, especially in the context of multi-class classification? The “Naive” part of Naive Bayes comes from the assumption of feature independence. It assumes that the features are conditionally independent of each other given the class label. This simplifies the computation and makes it computationally efficient.</p>
<p>Recall the discriminant or decision function:</p>
<div class="math notranslate nohighlight">
\[\arg\max_k P(Y=k | X=x) = \arg\max_k \pi_k f_k(x).\]</div>
<p>In this equation, <span class="math notranslate nohighlight">\(f_k\)</span> represents the density function for a p-dimensional vector <span class="math notranslate nohighlight">\(x = (x_1, \dots, x_p)\)</span>. In Naive Bayes, we factorize the joint distribution function into individual one-dimensional distribution functions for these p elements. This concept can be seen as a way to introduce independence as a form of regularization for high-dimensional problems.</p>
<div class="math notranslate nohighlight">
\[f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \cdots \times f_{kp}(x_p)\]</div>
<p>We proceed by estimating the one-dimensional distribution function for each dimension and each class. For instance, for discrete features, we can histograms or count-based methods; for numerical features, we can use kernel density functions since it’s a one-dimensional problem. This approach is known as <strong>non-parametric Naive Bayes</strong>. Alternatively, for numerical features, we can employ a one-dimensional normal density function to estimate each of the <span class="math notranslate nohighlight">\(f_{kj}\)</span>, which is termed <strong>parametric Naive Bayes</strong>.</p>
<p>To test your understanding: When employing parametric Naive Bayes, how many parameters must we estimate? We indeed need p means, one for each dimension, and p variances, one for each dimension, totaling 2p parameters.</p>
<p>I’d like to emphasize a subtle issue that arises during the prediction stage of parametric Naive Bayes. When making predictions for a new test point <span class="math notranslate nohighlight">\(x^*\)</span>, we compute the following decision or discriminant function for each k:</p>
<div class="math notranslate nohighlight">
\[\begin{split}d_k(x^*) &amp;=  - \log \Big [ \pi_k \frac{1}{\sqrt{\sigma^2_{k1}}} e^{-\frac{(x^*_1 - \mu_{k1})^2}{2 \sigma^2_{k1}}} \cdots  \frac{1}{\sqrt{\sigma^2_{kp}}} e^{-\frac{(x^*_p - \mu_{kp})^2}{2 \sigma^2_{kp}}} \Big ] \\
&amp;=  - \log \pi_k  + \frac{1}{2} \sum_{j=1}^p \Big [ \log \sigma^2_{kj} + \frac{(x^*_j - \mu_{kj})^2}{\sigma_{kj}^2} \Big ]\end{split}\]</div>
<p>Some Naive Bayes packages may compute this term using the first line, i.e., by evaluating the normal probability density function (pdf) and then taking the logarithm. When <span class="math notranslate nohighlight">\(x^*\)</span> is far from the center, the normal pdf can become very small, leading to numerical issues when applying the logarithm. To mitigate this problem, many Naive Bayes packages truncate the value of the normal pdf, and then proceed to take the logarithm. However, this truncation can result in incorrect predictions. It’s important to note that we are primarily interested in the relative magnitudes of <span class="math notranslate nohighlight">\(d_k\)</span>, determining which k gives us the highest <span class="math notranslate nohighlight">\(d_k\)</span>. Truncation erases the differences among the <span class="math notranslate nohighlight">\(d_k\)</span> values. For instance, all <span class="math notranslate nohighlight">\(d_k\)</span> values may get truncated, but <span class="math notranslate nohighlight">\(d_1\)</span> might be the smallest among them.</p>
<p>See our code page for an illustration of this issue using the ‘naive’ command from the ‘klaR’ package in R on the ‘spam’ dataset. The <cite>GaussianNB</cite> command from the <cite>sklearn</cite> package in Python does not have this issue.</p>
<ul class="simple">
<li><p>Rcode: [<a class="reference external" href="https://liangfgithub.github.io/Rcode_W9_NaiveBayes.html">Rcode_W9_NaiveBayes</a>]</p></li>
<li><p>Python: [<a class="reference external" href="https://liangfgithub.github.io/Python_W9_NaiveBayes.html">Python_W9_NaiveBayes</a>]</p></li>
</ul>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w9_5_FDA.html" class="btn btn-neutral float-left" title="9.5. Fisher Discriminant Analysis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w9_7_summ.html" class="btn btn-neutral float-right" title="9.7. Summary" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>