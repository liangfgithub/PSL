<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>9.1. Introduction to Classification &mdash; PSL_Book 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=2709fde1"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="9.2. Discriminant Analysis" href="w9_2_DA.html" />
    <link rel="prev" title="9. Discriminant Analysis" href="w9_index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            PSL_Book
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../w1/w1_index.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w2/w2_index.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w3/w3_index.html">3. Variable Selection and Regularization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w4/w4_index.html">4. Regression Trees and Ensemble</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w5/w5_index.html">5. Nonlinear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w6/w6_index.html">6. Clustering Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w7/w7_index.html">7. Latent Structure Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w8/w8_index.html">8. TBA</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="w9_index.html">9. Discriminant Analysis</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">9.1. Introduction to Classification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#steps-to-develop-a-classifier">9.1.1. Steps to Develop a Classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="#optimal-classifier">9.1.2. Optimal Classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="#decision-boundaries">9.1.3. Decision Boundaries</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="w9_2_DA.html">9.2. Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_3_QDA.html">9.3. Quadratic Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_4_LDA.html">9.4. Linear Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_5_FDA.html">9.5. Fisher Discriminant Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_6_NB.html">9.6. Naive Bayes Classifiers</a></li>
<li class="toctree-l2"><a class="reference internal" href="w9_7_summ.html">9.7. Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../w10/w10_index.html">10. Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w11/w11_index.html">11. Support Vector Machine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w12/w12_index.html">12. Classification Trees and Boosting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../w13/w13_index.html">13. Recommender System</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PSL_Book</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="w9_index.html"><span class="section-number">9. </span>Discriminant Analysis</a></li>
      <li class="breadcrumb-item active"><span class="section-number">9.1. </span>Introduction to Classification</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="introduction-to-classification">
<h1><span class="section-number">9.1. </span>Introduction to Classification<a class="headerlink" href="#introduction-to-classification" title="Link to this heading"></a></h1>
<p>Over the next few weeks, we will transition our focus to <strong>classification</strong> problems. But first, what exactly is a classification problem?</p>
<p>Imagine having <span class="math notranslate nohighlight">\(n\)</span> observations, each consisting of <span class="math notranslate nohighlight">\(p\)</span> measurements or features. These observations belong to different distinct classes, making this different from regression problems where our goal is to predict a numerical outcome. In classification, our goal is to predict class labels. To simplify our discussion, we’ll consider <strong>binary</strong> classification where there are just <strong>two</strong> classes.</p>
<p>Our aim is to establish a classification rule. This rule will use the <span class="math notranslate nohighlight">\(p\)</span> measurements as input and yield the class label as its output. We not only want this rule to be accurate for our training data but also for future, unseen observations.</p>
<p>Consider these examples:</p>
<ul class="simple">
<li><p>Lending Club Project: Here, <span class="math notranslate nohighlight">\(p\)</span> measurements could be various details about the loan or the borrower. Our goal? Predict if a loan was defaulted or paid off.</p></li>
<li><p>Sentiment Analysis Project: The <span class="math notranslate nohighlight">\(p\)</span> measurements could relate to aspects of a movie review, and our objective is to determine if the review is positive or negative.</p></li>
</ul>
<section id="steps-to-develop-a-classifier">
<h2><span class="section-number">9.1.1. </span>Steps to Develop a Classifier<a class="headerlink" href="#steps-to-develop-a-classifier" title="Link to this heading"></a></h2>
<p>To learn a classifier, we follow a set of steps that are similar to those we’ve used for regression problems. Here’s our typical approach for solving supervised machine learning problems:</p>
<ol class="arabic simple">
<li><p><strong>Data Collection</strong>: Collect training samples, denoted by  <span class="math notranslate nohighlight">\(x_i\)</span> (for p-dimensionalfeatures) and <span class="math notranslate nohighlight">\(y_i\)</span> (for binary response). For our discussions, assume the two classes are labeled 0 and 1.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[(x_i, y_i)_{i=1}^n, \quad x_i \in \mathbb{R}^p, \ y_i \in \{0, 1\}\]</div>
<ol class="arabic simple" start="2">
<li><p><strong>Function Selection</strong>: Choose a collection of functions <span class="math notranslate nohighlight">\(f\)</span> that take ‘p’ measurements as input and return a binary response. We can index these functions using a parameter <span class="math notranslate nohighlight">\(\theta\)</span>. For instance, if we decide on a linear function, the parameter <span class="math notranslate nohighlight">\(\theta\)</span> would be the linear coefficients.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[f: \mathbb{R}^p \longrightarrow \{0, 1\}\]</div>
<ol class="arabic simple" start="3">
<li><p><strong>Loss Function</strong>: Define a loss function <span class="math notranslate nohighlight">\(L(f(x), y)\)</span> that quantifies how well our prediction <span class="math notranslate nohighlight">\(f(x)\)</span> aligns with the true response <span class="math notranslate nohighlight">\(y\)</span>. In the case of binary classification, a commonly used loss function is the 0-1 loss:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[L(f(x), y) = 0, \text { if } y=f(x); \quad 1, \text { if } y \ne f(x).\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Optimization</strong>: Optimize the chosen loss function by minimizing the average loss over the ‘n’ training samples. This process leads us to the optimal classifier <span class="math notranslate nohighlight">\(f\)</span> that minimizes the empirical loss.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\min_f \frac{1}{n} \sum_{i=1}^n L(f(x_i), y_i)\]</div>
</section>
<section id="optimal-classifier">
<h2><span class="section-number">9.1.2. </span>Optimal Classifier<a class="headerlink" href="#optimal-classifier" title="Link to this heading"></a></h2>
<p>Let’s imagine an ideal scenario where we have an infinite number of samples, or in other words, we completely understand how our data <span class="math notranslate nohighlight">\((X, Y)\)</span> is generated. In this situation, there’s no distinction between training and test samples, as we possess complete knowledge of the data. As our sample size ‘n’ approaches infinity, our loss function tends toward its expectation. This expectation is computed with respect to the true data-generating process, which involves a joint distribution over both X and Y. The expected loss in this context is often referred to as the “risk” function of the classifier <span class="math notranslate nohighlight">\(f\)</span>.</p>
<div class="math notranslate nohighlight">
\[\frac{1}{n} \sum_{i=1}^n L(f(x_i), y_i) \to \mathbb{E} {X, Y} L(f(X), Y) := \text{Risk}[f]\]</div>
<p>It’s logical to then define the optimal classifier, <span class="math notranslate nohighlight">\(f^*\)</span>, as the function that minimizes this risk function.</p>
<div class="math notranslate nohighlight">
\[f^* = \arg\min_f  \text{Risk}[f].\]</div>
<p>Assuming we adopt the 0-1 loss function and permit the use of any function <span class="math notranslate nohighlight">\(f\)</span>, the next question arises: What exactly is this optimal <span class="math notranslate nohighlight">\(f^*\)</span>?</p>
<p>The risk function involves an expectation over the joint distribution of features X and the target variable Y. We can factorize the joint distribution as the product of the marginal of X and the conditional of Y given X. This simplifies the risk function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{Risk}[f] &amp; =  \mathbb{E}_{X, Y} L(f(X), Y) =  \int_{\mathcal{X}}  \int_{\mathcal{Y}} L(y, f(x)) p(x, y) dy dx \\
&amp;= \int_{\mathcal{X}}  \int_{\mathcal{Y}} L(y, f(x)) p(y | x ) p(x)   dy dx \\
&amp;= \int_{\mathcal{X}} \Big [ \textcolor{blue}{\int_{\mathcal{Y}} L(y, f(x)) p(y| x)  dy} \Big ] p(x) d x\end{split}\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(x)\)</span> is the marginal distribution function of <span class="math notranslate nohighlight">\(X\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(p(y | x)\)</span> is the conditional distribution function of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X=x\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(p(x) p(y|x)\)</span> is the joint distribution function of <span class="math notranslate nohighlight">\((X, Y)\)</span>.</p></li>
</ul>
<p>The challenge of finding the optimal <span class="math notranslate nohighlight">\(f^*\)</span> that minimizes the risk function can be broken down into a series of subproblems.</p>
<ol class="arabic simple">
<li><p>The inner integration (in blue) gives us the expected loss at a specific <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p>The outer integration then calculates the average of these expectations over all possible values of <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
</ol>
<p>This implies that if we find an <span class="math notranslate nohighlight">\(f\)</span> that minimizes the expected loss at each specific <span class="math notranslate nohighlight">\(x\)</span>, its average expected loss (i.e., the risk) will also be minimized.</p>
<p>Since <span class="math notranslate nohighlight">\(Y\)</span> is a discrete random variable taking only two possible values. The inside expectation integral transforms into a summation due to the discrete nature of <span class="math notranslate nohighlight">\(Y\)</span>. The distribution of <span class="math notranslate nohighlight">\(Y\)</span> given <span class="math notranslate nohighlight">\(X=x\)</span> follows a Bernoulli distribution with parameter <span class="math notranslate nohighlight">\(P(Y=1|X=x) := \eta(x)\)</span>.</p>
<p>Evaluate the expectation involves summing up probabilities for both <span class="math notranslate nohighlight">\(Y = 1\)</span> and <span class="math notranslate nohighlight">\(Y = 0\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\int_{\mathcal{Y}} L(y, f(x)) p(y| x)  dy &amp;= L(1, f(x)) \cdot P(Y=1|x) + L(0, f(x)) \cdot P(Y=0|x) \\
&amp;= L(1, f(x)) \cdot \eta(x) + L(0, f(x)) \cdot (1 - \eta(x)) \\
&amp;=   \begin{cases}
                1 - \eta(x), &amp;  \text{ if } f(x) = 1 \\
                \eta(x), &amp;  \text{ if } f(x) = 0 \end{cases}\end{split}\]</div>
<p>Thus, the optimal classifier hinges on  <span class="math notranslate nohighlight">\(\eta(x)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f^*(x) = \arg\min_f R[f] = \begin{cases}
                1 , &amp;  \text{ if } \eta(x) \ge 0.5 \\
                0, &amp; \text{ if }  \eta(x) &lt; 0.5 \end{cases}\end{split}\]</div>
<p>This optimal classifier, often known as the “Bayes rule,” minimizes the 0-1 loss and corresponds to the “Bayes risk” or “Bayes error.” We confirmed this computation during Coding Assignment 1. The above logic extends to multi-class problems, where ‘Y’ can take K different values. In such cases, the optimal classifier predicts the class label with the highest conditional probability:</p>
<div class="math notranslate nohighlight">
\[f^*(x) = \arg\max_k P(Y=k | X=x).\]</div>
</section>
<section id="decision-boundaries">
<h2><span class="section-number">9.1.3. </span>Decision Boundaries<a class="headerlink" href="#decision-boundaries" title="Link to this heading"></a></h2>
<p>A helpful way to comprehend classifiers is by visualizing decision boundaries. The decision boundary defines where the classifier switches its prediction. Specifically, for the Bayes classifier using 0-1 loss, the decision boundary is where the conditional probability is exactly 0.5. In cases where the decision boundary is linear, we refer to the classifier as a linear classifier.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="w9_index.html" class="btn btn-neutral float-left" title="9. Discriminant Analysis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="w9_2_DA.html" class="btn btn-neutral float-right" title="9.2. Discriminant Analysis" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Feng Liang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>